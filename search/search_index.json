{"config":{"lang":["en","fr","es","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"architecture/","title":"\ud83c\udfdb\ufe0f Architecture","text":""},{"location":"architecture/#technical-architecture","title":"\ud83c\udfd7\ufe0f Technical Architecture","text":"<ul> <li>Orchestration: GitHub Actions</li> <li>Data Warehouse: Snowflake</li> <li>Transformation: dbt</li> <li>Language: Python </li> </ul>"},{"location":"architecture/#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<pre><code>nyc-taxi-pipeline/\n\u251c\u2500\u2500 .github/\n\u2502   \u251c\u2500\u2500 workflows/\n\u2502   \u2502   \u251c\u2500\u2500 nyc_taxi_pipeline.yml\n\u2502   \u2502   \u251c\u2500\u2500 codeql.yml\n\u2502   \u2502   \u251c\u2500\u2500 python_code_tests.yml\n\u2502   \u2502   \u251c\u2500\u2500 release.yml\n\u2502   \u2502   \u2514\u2500\u2500 sqlfluff.yml\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 dependabot.yml\n\u2502\n\u251c\u2500\u2500 docs/\n\u2502\n\u251c\u2500\u2500 snowflake_ingestion/\n\u2502   \u251c\u2500\u2500 init_data_warehouse.py\n\u2502   \u251c\u2500\u2500 scrape_links.py\n\u2502   \u251c\u2500\u2500 upload_stage.py\n\u2502   \u251c\u2500\u2500 load_to_table.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 sql/\n\u2502   \u2502   \u251c\u2500\u2500 init/\n\u2502   \u2502   \u251c\u2500\u2500 scraping/\n\u2502   \u2502   \u251c\u2500\u2500 stage/\n\u2502   \u2502   \u2514\u2500\u2500 load/\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 tests/\n\u2502\n\u2514\u2500\u2500 dbt_transformations/\n    \u2514\u2500\u2500 NYC_Taxi_dbt/\n        \u2514\u2500\u2500 models/\n            \u251c\u2500\u2500 staging/\n            \u251c\u2500\u2500 final/\n            \u2514\u2500\u2500 marts/\n</code></pre>"},{"location":"architecture/#processing-flow","title":"\ud83d\udcca Processing Flow","text":""},{"location":"architecture/#main-pipeline","title":"Main Pipeline","text":"<p>NYC Taxi Data Pipeline Monthly execution data ingestion pipeline: </p> <ol> <li>Snowflake Infra Init    Initialization of Snowflake infrastructure (database, schemas, warehouse, role, user).</li> <li>Scrape Links    Scraping and retrieval of source links.</li> <li>Upload to Stage    Uploading raw files to Snowflake stage.</li> <li>Load to Table    Loading data into the RAW schema table.</li> <li>Run dbt Transformations    dbt transformations (STAGING then FINAL).</li> <li>Run dbt Tests    Execution of dbt tests to validate models.</li> <li>Backup Policy    Automatic configuration of backup policies for the database, RAW table, and FINAL schema.</li> </ol>"},{"location":"architecture/#quality-pipelines","title":"Quality Pipelines","text":"<ul> <li>CodeQL Security Scan   Static analysis of Python code using CodeQL to detect vulnerabilities on every push or pull request to <code>dev</code> and <code>main</code>.</li> <li>Dependabot Updates   Automated updates of Python and GitHub Actions dependencies on a quarterly schedule.</li> <li>pages-build-deployment   Automatic deployment of project documentation via GitHub Pages.</li> <li>Python Code Tests   Execution of Pytest unit tests on every push or pull request to <code>dev</code> and <code>main</code>.</li> <li>Release   Automatic versioning, changelog generation, and release publishing via Python Semantic Release on every push or pull request to <code>main</code>.</li> <li>SQL Code Quality   Automatic linting of SQL code (dbt models and Snowflake scripts) with SQLFluff on every push or pull request to <code>dev</code> and <code>main</code>.</li> </ul>"},{"location":"architecture/#data-modeling","title":"Data Modeling","text":"<p>This table documents how the data is stored.</p> Table Name Schema Table Type Materialization FILE_LOADING_METADATA <code>SCHEMA_RAW</code> Transient Table YELLOW_TAXI_TRIPS_RAW <code>SCHEMA_RAW</code> Permanent Incremental TAXI_ZONE_LOOKUP <code>SCHEMA_RAW</code> Permanent Table TAXI_ZONE_STG <code>SCHEMA_STG</code> Transient Table YELLOW_TAXI_TRIPS_STG <code>SCHEMA_STG</code> Transient Incremental int_trip_metrics <code>SCHEMA_STG</code> View fact_trips <code>SCHEMA_FINAL</code> Permanent Incremental dim_locations <code>SCHEMA_FINAL</code> Permanent Table dim_time <code>SCHEMA_FINAL</code> Permanent Table dim_date <code>SCHEMA_FINAL</code> Permanent Table marts <code>SCHEMA_FINAL</code> View <p>Details available in the \ud83d\udcda Online dbt documentation</p>"},{"location":"docstrings/","title":"Docstrings Python","text":""},{"location":"docstrings/#snowflake_ingestion.functions","title":"<code>snowflake_ingestion.functions</code>","text":""},{"location":"docstrings/#snowflake_ingestion.functions.config_logger","title":"<code>config_logger()</code>","text":"<p>Configure the global logger.</p> <p>Reads LOGGER_LEVEL from the module environment and configures the root logging settings (level, format, date format). Intended to be called once at application start.</p> <p>No return value.</p> Source code in <code>snowflake_ingestion/functions.py</code> <pre><code>def config_logger() -&gt; None:\n    \"\"\"Configure the global logger.\n\n    Reads LOGGER_LEVEL from the module environment and configures the\n    root logging settings (level, format, date format). Intended to be\n    called once at application start.\n\n    No return value.\n    \"\"\"\n    logging.basicConfig(\n        level=LOGGER_LEVEL,\n        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.functions.connect_with_role","title":"<code>connect_with_role(user, password, account, role)</code>","text":"<p>Create a Snowflake connection using the specified credentials and role.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>str</code> <p>Snowflake username.</p> required <code>password</code> <code>str</code> <p>Snowflake password.</p> required <code>account</code> <code>str</code> <p>Snowflake account identifier.</p> required <code>role</code> <code>str</code> <p>Snowflake role to assume for the session.</p> required <p>Returns:</p> Type Description <code>SnowflakeConnection</code> <p>snowflake.connector.connection.SnowflakeConnection: A Snowflake connection object with autocommit enabled.</p> Notes <p>This function opens a network connection to Snowflake. The caller is responsible for closing the connection when it is no longer needed.</p> Source code in <code>snowflake_ingestion/functions.py</code> <pre><code>def connect_with_role(user: str, password: str, account: str, role: str) -&gt; snowflake.connector.SnowflakeConnection:\n    \"\"\"Create a Snowflake connection using the specified credentials and role.\n\n    Args:\n        user (str): Snowflake username.\n        password (str): Snowflake password.\n        account (str): Snowflake account identifier.\n        role (str): Snowflake role to assume for the session.\n\n    Returns:\n        snowflake.connector.connection.SnowflakeConnection:\n            A Snowflake connection object with autocommit enabled.\n\n    Notes:\n        This function opens a network connection to Snowflake. The caller\n        is responsible for closing the connection when it is no longer needed.\n    \"\"\"\n    return snowflake.connector.connect(\n        user=user,\n        password=password,\n        account=account,\n        role=role,\n        autocommit=True,\n    )\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.functions.plural_suffix","title":"<code>plural_suffix(count)</code>","text":"<p>Return 's' if count is greater than or equal to 2, else return an empty string.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>The number of items.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>'s' if count &gt;= 2, else ''.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; plural_suffix(0)\n''\n&gt;&gt;&gt; plural_suffix(1)\n''\n&gt;&gt;&gt; plural_suffix(2)\n's'\n&gt;&gt;&gt; plural_suffix(3)\n's'\n</code></pre> Source code in <code>snowflake_ingestion/functions.py</code> <pre><code>def plural_suffix(count: int) -&gt; str:\n    \"\"\"Return 's' if count is greater than or equal to 2, else return an empty string.\n\n    Args:\n        count (int): The number of items.\n\n    Returns:\n        str: 's' if count &gt;= 2, else ''.\n\n    Examples:\n        &gt;&gt;&gt; plural_suffix(0)\n        ''\n        &gt;&gt;&gt; plural_suffix(1)\n        ''\n        &gt;&gt;&gt; plural_suffix(2)\n        's'\n        &gt;&gt;&gt; plural_suffix(3)\n        's'\n    \"\"\"\n    return \"s\" if count &gt;= 2 else \"\"\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.functions.run_sql_file","title":"<code>run_sql_file(cur, filepath)</code>","text":"<p>Execute SQL statements from a file using VAR_PLACEHOLDER placeholders.</p> The function <ul> <li>Reads the SQL file.</li> <li>Finds placeholders in the form VAR_PLACEHOLDER (captures VAR).</li> <li>Replaces each found placeholder with the value of the corresponding   global variable named VAR (stringified).</li> <li>Masks variables containing \"PASSWORD\" in logger output.</li> <li>Splits the file by semicolons and executes non-empty statements.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>cur</code> <code>SnowflakeCursor</code> <p>Active cursor.</p> required <code>filepath</code> <code>Path or str</code> <p>Path to the SQL file.</p> required Notes <p>Placeholders that do not match a global variable are replaced with a string of the form <code>&lt;VAR_NOT_FOUND&gt;</code>.</p> Source code in <code>snowflake_ingestion/functions.py</code> <pre><code>def run_sql_file(cur: snowflake.connector.cursor.SnowflakeCursor, filepath: Path | str) -&gt; None:\n    \"\"\"Execute SQL statements from a file using VAR_PLACEHOLDER placeholders.\n\n    The function:\n      - Reads the SQL file.\n      - Finds placeholders in the form VAR_PLACEHOLDER (captures VAR).\n      - Replaces each found placeholder with the value of the corresponding\n        global variable named VAR (stringified).\n      - Masks variables containing \"PASSWORD\" in logger output.\n      - Splits the file by semicolons and executes non-empty statements.\n\n    Args:\n        cur (snowflake.connector.cursor.SnowflakeCursor): Active cursor.\n        filepath (pathlib.Path or str): Path to the SQL file.\n\n    Notes:\n        Placeholders that do not match a global variable are replaced with\n        a string of the form `&lt;VAR_NOT_FOUND&gt;`.\n    \"\"\"\n    with open(filepath, \"r\") as f:\n        sql = f.read()\n        keys = re.findall(r'(?:SCHEMA_)?(\\w+)_PLACEHOLDER', sql)\n        variables = {k: globals().get(k, f\"&lt;{k}_NOT_FOUND&gt;\") for k in keys}\n        logger.debug(f\"\ud83d\udd0e Variables d\u00e9tect\u00e9es dans {Path(filepath).name}: {sorted(set(keys))}\")\n        for key, value in variables.items():\n            sql = sql.replace(f\"{key}_PLACEHOLDER\", str(value))\n        masked_vars = {k: \"*****\" if \"PASSWORD\" in k.upper() else v for k, v in variables.items()}\n        logger.debug(f\"Variables utilis\u00e9es : {dict(sorted(masked_vars.items()))}\")\n\n        for statement in sql.split(\";\"):\n            statement = statement.strip()\n            if statement:\n                cur.execute(statement)\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.functions.use_context","title":"<code>use_context(cur, WH_NAME, DW_NAME, RAW_SCHEMA)</code>","text":"<p>Set the Snowflake session context: warehouse, database and schema.</p> <p>Parameters:</p> Name Type Description Default <code>cur</code> <code>SnowflakeCursor</code> <p>Active Snowflake cursor.</p> required <code>WH_NAME</code> <code>str</code> <p>Warehouse name to use.</p> required <code>DW_NAME</code> <code>str</code> <p>Database name to use.</p> required <code>RAW_SCHEMA</code> <code>str</code> <p>Schema name to use.</p> required <p>Raises:</p> Type Description <code>SystemExit</code> <p>Exits the process on any exception when setting the context.</p> Source code in <code>snowflake_ingestion/functions.py</code> <pre><code>def use_context(cur: snowflake.connector.cursor.SnowflakeCursor, WH_NAME: str, DW_NAME: str, RAW_SCHEMA: str) -&gt; None:\n    \"\"\"Set the Snowflake session context: warehouse, database and schema.\n\n    Args:\n        cur (snowflake.connector.cursor.SnowflakeCursor): Active Snowflake cursor.\n        WH_NAME (str): Warehouse name to use.\n        DW_NAME (str): Database name to use.\n        RAW_SCHEMA (str): Schema name to use.\n\n    Raises:\n        SystemExit: Exits the process on any exception when setting the context.\n    \"\"\"\n    logger.debug(f\"\u2699\ufe0f Configuration du contexte: WH={WH_NAME}, DB={DW_NAME}, SCHEMA=SCHEMA_{RAW_SCHEMA}\")\n    try:\n        cur.execute(f\"USE WAREHOUSE {WH_NAME}\")\n        cur.execute(f\"USE DATABASE {DW_NAME}\")\n        cur.execute(f\"USE SCHEMA SCHEMA_{RAW_SCHEMA}\")\n    except Exception as e:\n        logger.critical(\"\u274c Erreur : Relancer l'\u00e9tape Snowflake Infra Init\")\n        sys.exit(1)\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.init_infra_snowflake","title":"<code>snowflake_ingestion.init_infra_snowflake</code>","text":""},{"location":"docstrings/#snowflake_ingestion.init_infra_snowflake.create_roles_and_user","title":"<code>create_roles_and_user(cur)</code>","text":"<p>Create the DBT role and user in Snowflake.</p> <p>Parameters:</p> Name Type Description Default <code>cur</code> <code>SnowflakeCursor</code> <p>Active Snowflake cursor.</p> required Source code in <code>snowflake_ingestion/init_infra_snowflake.py</code> <pre><code>def create_roles_and_user(cur: SnowflakeCursor) -&gt; None:\n    \"\"\"Create the DBT role and user in Snowflake.\n\n    Args:\n        cur (snowflake.connector.cursor.SnowflakeCursor): Active Snowflake cursor.\n    \"\"\"\n    logger.info(\"\ud83d\udd10 Creating roles and users...\")\n    sql_file = SQL_DIR / \"create_roles_and_user.sql\"\n    functions.run_sql_file(cur, sql_file)\n    logger.info(\"\u2705 Roles and users created\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.init_infra_snowflake.grant_privileges","title":"<code>grant_privileges(cur)</code>","text":"<p>Grant required privileges to the TRANSFORMER role in Snowflake.</p> <p>Parameters:</p> Name Type Description Default <code>cur</code> <code>SnowflakeCursor</code> <p>Active Snowflake cursor.</p> required Source code in <code>snowflake_ingestion/init_infra_snowflake.py</code> <pre><code>def grant_privileges(cur: SnowflakeCursor) -&gt; None:\n    \"\"\"Grant required privileges to the TRANSFORMER role in Snowflake.\n\n    Args:\n        cur (snowflake.connector.cursor.SnowflakeCursor): Active Snowflake cursor.\n    \"\"\"\n    logger.info(\"\ud83d\udd11 Granting privileges to the roles...\")\n    sql_file = SQL_DIR / \"grant_privileges.sql\"\n    functions.run_sql_file(cur, sql_file)\n    logger.info(\"\u2705 Privileges granted\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.init_infra_snowflake.main","title":"<code>main()</code>","text":"<p>Main initialization process for the Snowflake environment.</p> <p>Establishes connections with appropriate roles (SYSADMIN, SECURITYADMIN, ACCOUNTADMIN) and executes setup steps in order.</p> Source code in <code>snowflake_ingestion/init_infra_snowflake.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main initialization process for the Snowflake environment.\n\n    Establishes connections with appropriate roles (SYSADMIN, SECURITYADMIN, ACCOUNTADMIN)\n    and executes setup steps in order.\n    \"\"\"\n    try:\n        conn = functions.connect_with_role(functions.USER, functions.PASSWORD, functions.ACCOUNT, \"SYSADMIN\")\n        with conn.cursor() as cur:\n            setup_data_warehouse(cur)\n        conn.close()\n\n        conn = functions.connect_with_role(functions.USER, functions.PASSWORD, functions.ACCOUNT, \"SECURITYADMIN\")\n        with conn.cursor() as cur:\n            create_roles_and_user(cur)\n            grant_privileges(cur)\n        conn.close()\n\n        conn = functions.connect_with_role(functions.USER, functions.PASSWORD, functions.ACCOUNT, \"ACCOUNTADMIN\")\n        with conn.cursor() as cur:\n            set_data_retention(cur)\n        conn.close()\n\n        logger.info(\"\ud83c\udfaf Complete initialization finished successfully!\")\n    except Exception as e:\n        logger.error(e)\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.init_infra_snowflake.set_data_retention","title":"<code>set_data_retention(cur)</code>","text":"<p>Set the data retention period for the Snowflake account.</p> <p>Checks if the account is Enterprise, then applies the retention time. Logs the result in days, with pluralization handled automatically.</p> <p>Parameters:</p> Name Type Description Default <code>cur</code> <code>SnowflakeCursor</code> <p>Active Snowflake cursor.</p> required Source code in <code>snowflake_ingestion/init_infra_snowflake.py</code> <pre><code>def set_data_retention(cur: SnowflakeCursor) -&gt; None:\n    \"\"\"Set the data retention period for the Snowflake account.\n\n    Checks if the account is Enterprise, then applies the retention time.\n    Logs the result in days, with pluralization handled automatically.\n\n    Args:\n        cur (snowflake.connector.cursor.SnowflakeCursor): Active Snowflake cursor.\n    \"\"\"\n    logger.info(\"\ud83c\udfd7\ufe0f  Setting up data retention\")\n    sql_file = SQL_DIR / \"set_data_retention.sql\"\n    functions.run_sql_file(cur, sql_file)\n    s = functions.plural_suffix(int(functions.RETENTION_TIME))\n    logger.info(f\"\u2705 Data retention set to {functions.RETENTION_TIME} day{s}\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.init_infra_snowflake.setup_data_warehouse","title":"<code>setup_data_warehouse(cur)</code>","text":"<p>Create the data warehouse, database, and schemas in Snowflake.</p> <p>Parameters:</p> Name Type Description Default <code>cur</code> <code>SnowflakeCursor</code> <p>Active Snowflake cursor.</p> required Source code in <code>snowflake_ingestion/init_infra_snowflake.py</code> <pre><code>def setup_data_warehouse(cur: SnowflakeCursor) -&gt; None:\n    \"\"\"Create the data warehouse, database, and schemas in Snowflake.\n\n    Args:\n        cur (snowflake.connector.cursor.SnowflakeCursor): Active Snowflake cursor.\n    \"\"\"\n    logger.info(\"\ud83c\udfd7\ufe0f  Creating warehouse, database and schemas...\")\n    sql_file = SQL_DIR / \"setup_data_warehouse.sql\"\n    functions.run_sql_file(cur, sql_file)\n    logger.info(\"\u2705 Warehouse and schemas created\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.scrape_links","title":"<code>snowflake_ingestion.scrape_links</code>","text":""},{"location":"docstrings/#snowflake_ingestion.scrape_links.get_parquet_links","title":"<code>get_parquet_links()</code>","text":"<p>Scrape the NYC Taxi data page for Parquet file URLs. Sends an HTTP request to the NYC Taxi,parses the page HTML, and extracts links to Parquet files for the relevant years.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>list[str]: List of Parquet file URLs.</p> Source code in <code>snowflake_ingestion/scrape_links.py</code> <pre><code>def get_parquet_links() -&gt; List[str]:\n    \"\"\"Scrape the NYC Taxi data page for Parquet file URLs.\n    Sends an HTTP request to the NYC Taxi,parses the page HTML,\n    and extracts links to Parquet files for the relevant years.\n\n    Returns:\n        list[str]: List of Parquet file URLs.\n    \"\"\"\n    logger.info(\"\ud83c\udf10 Starting NYC Taxi data scraping\")\n    response = requests.get(scraping_url)\n    tree = html.fromstring(response.content)\n    xpath_query = get_xpath()\n    filtered_links = tree.xpath(xpath_query)\n    return [\n        link.get(\"href\")\n        for link in filtered_links\n        if link.get(\"href\") and link.get(\"href\").endswith(\".parquet\")\n    ]\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.scrape_links.get_scraping_year","title":"<code>get_scraping_year()</code>","text":"<p>Determine the scraping year to use based on environment settings. Uses SCRAPING_YEAR if defined and valid, otherwise selects the previous year when current month \u2264 3, or the current year otherwise.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The year to scrape.</p> <p>Doctests: from functions import SCRAPING_YEAR</p> <p>get_scraping_year() == (int(SCRAPING_YEAR) if SCRAPING_YEAR != '' else current_year) - int(current_month &lt;= 3) True</p> Source code in <code>snowflake_ingestion/scrape_links.py</code> <pre><code>def get_scraping_year() -&gt; int:\n    \"\"\"Determine the scraping year to use based on environment settings.\n    Uses SCRAPING_YEAR if defined and valid, otherwise selects the previous\n    year when current month \u2264 3, or the current year otherwise.\n\n    Returns:\n        int: The year to scrape.\n\n    Doctests:\n    from functions import SCRAPING_YEAR\n    &gt;&gt;&gt; get_scraping_year() == (int(SCRAPING_YEAR) if SCRAPING_YEAR != '' else current_year) - int(current_month &lt;= 3)\n    True\n\n    \"\"\"\n    default_year = current_year - 1 if current_month &lt;= 3 else current_year\n    if functions.SCRAPING_YEAR == \"\":\n        return default_year\n    else:\n        try:\n            int_year = int(functions.SCRAPING_YEAR)\n        except ValueError:\n            logger.error(f\"\\\"SCRAPING_YEAR = {functions.SCRAPING_YEAR}\\\" is not a valid year!\")\n            logger.warning(f\"Scraping year has been reset to {default_year}\")\n            return default_year\n\n        if int_year &lt; 2009 or int_year &gt; current_year:\n            logger.error(\n                f\"\\\"SCRAPING_YEAR = {functions.SCRAPING_YEAR}\\\" scraping year must be between 2009 and {current_year} inclusive!\"\n            )\n            logger.warning(f\"Scraping year has been reset to {default_year}\")\n            return default_year\n        logger.info(f\"Files will be scraped from year {default_year}\")\n        return int_year\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.scrape_links.get_xpath","title":"<code>get_xpath()</code>","text":"<p>Build the XPath expression used to locate Parquet file links. The expression filters NYC Taxi data links by year, starting from the scraping year up to the current year.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>XPath query string.</p> Source code in <code>snowflake_ingestion/scrape_links.py</code> <pre><code>def get_xpath() -&gt; str:\n    \"\"\"Build the XPath expression used to locate Parquet file links.\n    The expression filters NYC Taxi data links by year, starting from the\n    scraping year up to the current year.\n\n    Returns:\n        str: XPath query string.\n    \"\"\"\n    xpath_query = \"//a[@title='Yellow Taxi Trip Records' and (\"\n    get_contains = lambda year: f\"contains(@href, '{year}')\"\n    contains_list = [get_contains(year) for year in range(get_scraping_year(), current_year + 1)]\n    xpath_query += \" or \".join(contains_list) + \")]\"\n    return xpath_query\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.scrape_links.main","title":"<code>main()</code>","text":"<p>Main scraping and metadata update workflow. Connects to Snowflake using the transformer role, initializes context, checks or creates the metadata table, scrapes new file URLs, and updates the metadata accordingly.</p> Source code in <code>snowflake_ingestion/scrape_links.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main scraping and metadata update workflow.\n    Connects to Snowflake using the transformer role, initializes context,\n    checks or creates the metadata table, scrapes new file URLs, and updates\n    the metadata accordingly.\n    \"\"\"\n    conn = functions.connect_with_role(\n        functions.USER_DEV,\n        functions.PASSWORD_DEV,\n        functions.ACCOUNT,\n        functions.ROLE_TRANSFORMER,\n    )\n    with conn.cursor() as cur:\n        functions.use_context(cur, functions.WH_NAME, functions.DW_NAME, functions.RAW_SCHEMA)\n        setup_meta_table(cur)\n\n        links = get_parquet_links()\n        s = functions.plural_suffix(len(links))\n        logger.info(f\"\ud83d\udcce {len(links)} link{s} found\")\n        new_file_detected: bool = False\n\n        for url in links:\n            filename = url.split(\"/\")[-1]\n            cur.execute(\n                f\"SELECT 1 FROM {functions.METADATA_TABLE} WHERE file_name = %s\",\n                (filename,),\n            )\n            if not cur.fetchone():\n                logger.info(f\"\u2795 New file detected : {filename}\")\n                new_file_detected = True\n\n                parts = (\n                    filename.replace(\"yellow_tripdata_\", \"\")\n                    .replace(\".parquet\", \"\")\n                    .split(\"-\")\n                )\n                year = int(parts[0]) if len(parts) &gt; 0 else None\n                month = int(parts[1]) if len(parts) &gt; 1 else None\n\n                logger.debug(f\"\ud83d\ude80 Loading {functions.METADATA_TABLE}\")\n                cur.execute(\n                    f\"\"\"\n                    INSERT INTO {functions.METADATA_TABLE}\n                    (file_url, file_name, year, month, rows_loaded, load_status)\n                    VALUES (%s, %s, %s, %s, 0, 'SCRAPED')\n                    \"\"\",\n                    (url, filename, year, month),\n                )\n            else:\n                logger.info(f\"\u23ed\ufe0f  {filename} already referenced\")\n\n            if not new_file_detected:\n                logger.debug(\"\ud83d\udd0d Analyzing SCRAPED files\")\n                functions.run_sql_file(cur, SQL_DIR / \"count_new_files.sql\")\n                if cur.fetchone()[0] &gt; 0:\n                    new_file_detected = True\n\n    conn.close()\n\n    if not new_file_detected:\n        logger.warning(\"\u26a0\ufe0f  No new files to load.\")\n\n    logger.info(\"\u2705 Scraping completed\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.scrape_links.setup_meta_table","title":"<code>setup_meta_table(cur)</code>","text":"<p>Ensure the metadata table exists in Snowflake. Executes the SQL script responsible for creating or verifying the metadata table.</p> <p>Parameters:</p> Name Type Description Default <code>cur</code> <code>SnowflakeCursor</code> <p>Active Snowflake cursor.</p> required Source code in <code>snowflake_ingestion/scrape_links.py</code> <pre><code>def setup_meta_table(cur: SnowflakeCursor) -&gt; None:\n    \"\"\"Ensure the metadata table exists in Snowflake.\n    Executes the SQL script responsible for creating or verifying the\n    metadata table.\n\n    Args:\n        cur (snowflake.connector.cursor.SnowflakeCursor): Active Snowflake cursor.\n    \"\"\"\n    logger.info(\"\ud83d\udccb Verification/Creation of metadata table\")\n    sql_file = SQL_DIR / \"setup_meta_table.sql\"\n    functions.run_sql_file(cur, sql_file)\n    logger.info(\"\u2705 Metadata table ready\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.upload_stage","title":"<code>snowflake_ingestion.upload_stage</code>","text":""},{"location":"docstrings/#snowflake_ingestion.upload_stage.download_and_upload_file","title":"<code>download_and_upload_file(cur, file_url, filename)</code>","text":"<p>Download a Parquet file from URL and upload it directly to Snowflake stage.</p> <p>This function streams the file content directly to Snowflake without persisting it permanently on disk. It uses a temporary file that is automatically deleted after the upload completes, ensuring no residual files are left behind.</p> <p>Parameters:</p> Name Type Description Default <code>cur</code> <code>SnowflakeCursor</code> <p>Active Snowflake cursor  used to execute the PUT command.</p> required <code>file_url</code> <code>str</code> <p>HTTPS URL of the Parquet file to download.</p> required <code>filename</code> <code>str</code> <p>Destination filename in the Snowflake stage.</p> required <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the HTTP request fails (non-200 status code).</p> <code>Error</code> <p>If the Snowflake PUT command fails.</p> Source code in <code>snowflake_ingestion/upload_stage.py</code> <pre><code>def download_and_upload_file(cur: SnowflakeCursor, file_url: str, filename: str) -&gt; None:\n    \"\"\"Download a Parquet file from URL and upload it directly to Snowflake stage.\n\n    This function streams the file content directly to Snowflake without persisting\n    it permanently on disk. It uses a temporary file that is automatically deleted\n    after the upload completes, ensuring no residual files are left behind.\n\n    Args:\n        cur (snowflake.connector.cursor.SnowflakeCursor): Active Snowflake cursor \n            used to execute the PUT command.\n        file_url (str): HTTPS URL of the Parquet file to download.\n        filename (str): Destination filename in the Snowflake stage.\n\n    Raises:\n        requests.HTTPError: If the HTTP request fails (non-200 status code).\n        snowflake.connector.errors.Error: If the Snowflake PUT command fails.\n    \"\"\"\n    logger.info(f\"\ud83d\udce5 Downloading {filename}...\")\n    response = requests.get(file_url)\n    response.raise_for_status()\n    with tempfile.NamedTemporaryFile(suffix=\".parquet\", delete=True) as tmp_file:\n        tmp_file.write(response.content)\n        tmp_file.flush()\n        logger.info(\"\ud83d\udce4 Uploading to Snowflake...\")\n        cur.execute(f\"PUT 'file://{tmp_file.name}' @~/{filename} AUTO_COMPRESS=FALSE\")\n    logger.info(f\"\u2705 {filename} uploaded and temporary file cleaned\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.upload_stage.main","title":"<code>main()</code>","text":"<p>Main staging process for Parquet files.</p> <p>Connects to Snowflake, retrieves metadata for scraped files, downloads each file, uploads it to the stage, and updates the metadata table with the appropriate load status.</p> Source code in <code>snowflake_ingestion/upload_stage.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main staging process for Parquet files.\n\n    Connects to Snowflake, retrieves metadata for scraped files, downloads\n    each file, uploads it to the stage, and updates the metadata table\n    with the appropriate load status.\n    \"\"\"\n    conn = functions.connect_with_role(\n        functions.USER_DEV,\n        functions.PASSWORD_DEV,\n        functions.ACCOUNT,\n        functions.ROLE_TRANSFORMER,\n    )\n\n    with conn.cursor() as cur:\n        functions.use_context(cur, functions.WH_NAME, functions.DW_NAME, functions.RAW_SCHEMA)\n        logger.debug(\"\ud83d\udce5 Retrieving scraped file URLs and names\")\n        functions.run_sql_file(cur, SQL_DIR / \"select_file_url_name_from_meta_scraped.sql\")\n        scraped_files = cur.fetchall()\n        scraped_files_count: int = len(scraped_files)\n\n        if scraped_files_count == 0:\n            logger.warning(\"\u26a0\ufe0f  No files to upload\")\n        else:\n            logger.info(f\"\ud83d\udce6 {scraped_files_count} files to upload\")\n\n        for file_url, filename in scraped_files:\n            try:\n                download_and_upload_file(cur, file_url, filename)\n                logger.info(f\"\u2705 {filename} uploaded\")\n                cur.execute(\n                    f\"UPDATE {functions.METADATA_TABLE} SET load_status='STAGED' WHERE file_name=%s\",\n                    (filename,),\n                )\n                logger.debug(f\"\ud83d\ude80 Loading {functions.METADATA_TABLE}\")\n            except Exception as e:\n                logger.error(f\"\u274c Upload error {filename}: {e}\")\n                logger.debug(f\"\ud83d\ude80 Loading {functions.METADATA_TABLE}\")\n                cur.execute(\n                    f\"UPDATE {functions.METADATA_TABLE} SET load_status='FAILED_STAGE' WHERE file_name=%s\",\n                    (filename,),\n                )\n\n    conn.close()\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.load_to_table","title":"<code>snowflake_ingestion.load_to_table</code>","text":""},{"location":"docstrings/#snowflake_ingestion.load_to_table.cleanup_stage_file","title":"<code>cleanup_stage_file(cur, filename)</code>","text":"<p>Remove the processed file from the Snowflake stage. Args:     cur (snowflake.connector.cursor.SnowflakeCursor): Active Snowflake cursor.     filename (str): Name of the file to delete from the stage.</p> Source code in <code>snowflake_ingestion/load_to_table.py</code> <pre><code>def cleanup_stage_file(cur: SnowflakeCursor, filename: str) -&gt; None:\n    \"\"\"Remove the processed file from the Snowflake stage.\n    Args:\n        cur (snowflake.connector.cursor.SnowflakeCursor): Active Snowflake cursor.\n        filename (str): Name of the file to delete from the stage.\n    \"\"\"\n    cur.execute(f\"REMOVE @~/{filename}\")\n    logger.info(f\"\u2705 {filename} removed from stage\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.load_to_table.copy_file_to_table_and_count","title":"<code>copy_file_to_table_and_count(cur, filename, table_schema)</code>","text":"<p>Load a Parquet file from stage into the RAW table and count inserted rows. Uses COPY INTO with transformation to generate TRIP_ID using sequence and  maps Parquet columns using positional references.</p> <p>Parameters:</p> Name Type Description Default <code>cur</code> <code>SnowflakeCursor</code> <p>Active Snowflake cursor.</p> required <code>filename</code> <code>str</code> <p>Name of the staged file to load.</p> required <code>table_schema</code> <code>list</code> <p>Pre-detected schema from create_table function.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of rows inserted into the RAW table.</p> Source code in <code>snowflake_ingestion/load_to_table.py</code> <pre><code>def copy_file_to_table_and_count(cur: SnowflakeCursor, filename: str, table_schema: List[Tuple[str, str]]) -&gt; int:\n    \"\"\"Load a Parquet file from stage into the RAW table and count inserted rows.\n    Uses COPY INTO with transformation to generate TRIP_ID using sequence and \n    maps Parquet columns using positional references.\n\n    Args:\n        cur (snowflake.connector.cursor.SnowflakeCursor): Active Snowflake cursor.\n        filename (str): Name of the staged file to load.\n        table_schema (list): Pre-detected schema from create_table function.\n\n    Returns:\n        int: Number of rows inserted into the RAW table.\n    \"\"\"\n    logger.info(f\"\ud83d\ude80 Loading {filename} into {functions.RAW_TABLE}...\")\n    column_names = [col[0].replace(\"airport_fee\", \"Airport_fee\") for col in table_schema]\n    select_columns = [f\"$1:{col_name}\" for col_name in column_names]\n    copy_sql = f\"\"\"\n        COPY INTO {functions.RAW_TABLE} (TRIP_ID, {', '.join(column_names)}, FILENAME)\n        FROM (\n            SELECT \n                {functions.ID_SEQUENCE}.NEXTVAL,\n                {', '.join(select_columns)},\n                '{filename}'\n            FROM '@~/{filename}'\n        )\n        FILE_FORMAT=(FORMAT_NAME='{functions.DW_NAME}.SCHEMA_{functions.RAW_SCHEMA}.{functions.PARQUET_FORMAT}')\n        FORCE = TRUE\n    \"\"\"\n    cur.execute(copy_sql)\n    result = cur.fetchone()\n    if result and len(result) &gt; 3:\n        rows_loaded = result[3]\n    else:\n        rows_loaded = 0\n    s = functions.plural_suffix(rows_loaded)\n    logger.info(f\"\u2705 {filename} loaded ({rows_loaded} row{s})\")\n    return rows_loaded\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.load_to_table.create_table","title":"<code>create_table(cur)</code>","text":"<p>Create or verify the RAW table dynamically based on staged file schema. Executes SQL to detect the file schema in the Snowflake stage, creates the RAW table if it does not exist, and adds the filename column if needed.</p> <p>Parameters:</p> Name Type Description Default <code>cur</code> <code>SnowflakeCursor</code> <p>Active Snowflake cursor.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[Tuple[str, str]]</code> <p>The table schema detected from staged files</p> Source code in <code>snowflake_ingestion/load_to_table.py</code> <pre><code>def create_table(cur: SnowflakeCursor) -&gt; List[Tuple[str, str]]:\n    \"\"\"Create or verify the RAW table dynamically based on staged file schema.\n    Executes SQL to detect the file schema in the Snowflake stage,\n    creates the RAW table if it does not exist, and adds the filename\n    column if needed.\n\n    Args:\n        cur (snowflake.connector.cursor.SnowflakeCursor): Active Snowflake cursor.\n\n    Returns:\n        list: The table schema detected from staged files\n    \"\"\"\n    logger.info(f\"\ud83d\udccb Dynamic verification/creation of table {functions.RAW_TABLE}\")\n    functions.run_sql_file(cur, SQL_DIR / \"detect_file_schema_stage.sql\")\n    schema = cur.fetchall()\n    seen = set()\n    table_schema: List[Tuple[str, str]] = []\n    for col_name, col_type in schema:\n        if col_name.lower() not in seen:\n            seen.add(col_name.lower())\n            table_schema.append((col_name, col_type))\n    if len(table_schema) == 0:\n        logger.warning(\"\u26a0\ufe0f  No data in STAGE\")\n        return table_schema\n    functions.run_sql_file(cur, SQL_DIR / \"create_sequence.sql\")\n\n    columns = [f\"TRIP_ID NUMBER\"] + [f\"{col_name} {col_type}\" for col_name, col_type in table_schema]\n    if len(columns) != 0:\n        create_sql = f\"CREATE TABLE IF NOT EXISTS {functions.RAW_TABLE} ({', '.join(columns)})\"\n        cur.execute(create_sql)\n        functions.run_sql_file(cur, SQL_DIR / \"add_filename_to_raw_table.sql\")\n        logger.info(f\"\u2705 Table {functions.RAW_TABLE} ready\")\n    else:\n        logger.warning(f\"\u26a0\ufe0f  No data in STAGE\")\n    return table_schema\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.load_to_table.handle_loading_error","title":"<code>handle_loading_error(cur, filename, error)</code>","text":"<p>Handle errors occurring during file loading into the RAW table. Logs the error and updates the metadata table to mark the file as failed during the load step.</p> <p>Parameters:</p> Name Type Description Default <code>cur</code> <code>SnowflakeCursor</code> <p>Active Snowflake cursor.</p> required <code>filename</code> <code>str</code> <p>Name of the file that failed to load.</p> required <code>error</code> <code>Exception</code> <p>Exception raised during the loading process.</p> required Source code in <code>snowflake_ingestion/load_to_table.py</code> <pre><code>def handle_loading_error(cur: SnowflakeCursor, filename: str, error: Exception) -&gt; None:\n    \"\"\"Handle errors occurring during file loading into the RAW table.\n    Logs the error and updates the metadata table to mark the file\n    as failed during the load step.\n\n    Args:\n        cur (snowflake.connector.cursor.SnowflakeCursor): Active Snowflake cursor.\n        filename (str): Name of the file that failed to load.\n        error (Exception): Exception raised during the loading process.\n    \"\"\"\n    logger.error(f\"\u274c Loading error {filename}: {error}\")\n    logger.debug(f\"\ud83d\ude80 Loading {functions.METADATA_TABLE}\")\n    cur.execute(\n        f\"UPDATE {functions.METADATA_TABLE} SET load_status='FAILED_LOAD' WHERE file_name=%s\",\n        (filename,),\n    )\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.load_to_table.main","title":"<code>main()</code>","text":"<p>Main process for loading staged Parquet files into the RAW table. Connects to Snowflake, ensures the RAW table exists, retrieves staged files, loads each into the RAW table, updates metadata, and cleans up stage files.</p> Source code in <code>snowflake_ingestion/load_to_table.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main process for loading staged Parquet files into the RAW table.\n    Connects to Snowflake, ensures the RAW table exists, retrieves staged files,\n    loads each into the RAW table, updates metadata, and cleans up stage files.\n    \"\"\"\n    conn = functions.connect_with_role(functions.USER_DEV, functions.PASSWORD_DEV, functions.ACCOUNT, functions.ROLE_TRANSFORMER)\n    with conn.cursor() as cur:\n        functions.use_context(cur, functions.WH_NAME, functions.DW_NAME, functions.RAW_SCHEMA)\n        table_schema = create_table(cur)\n\n        logger.info(\"\ud83d\udd0d Analyzing files in STAGE\")\n        functions.run_sql_file(cur, SQL_DIR / \"select_filename_from_meta_staged.sql\")\n        staged_files = cur.fetchall()\n\n        for (filename,) in staged_files:\n            try:\n                rows_loaded = copy_file_to_table_and_count(cur, filename, table_schema)\n                update_metadata(cur, filename, rows_loaded)\n                cleanup_stage_file(cur, filename)\n            except Exception as e:\n                handle_loading_error(cur, filename, e)\n\n    conn.close()\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.load_to_table.update_metadata","title":"<code>update_metadata(cur, filename, rows_loaded)</code>","text":"<p>Update the metadata table after successful file loading. Args:     cur (snowflake.connector.cursor.SnowflakeCursor): Active Snowflake cursor.     filename (str): Name of the loaded file.     rows_loaded (int): Number of rows successfully inserted.</p> Source code in <code>snowflake_ingestion/load_to_table.py</code> <pre><code>def update_metadata(cur: SnowflakeCursor, filename: str, rows_loaded: int) -&gt; None:\n    \"\"\"Update the metadata table after successful file loading.\n    Args:\n        cur (snowflake.connector.cursor.SnowflakeCursor): Active Snowflake cursor.\n        filename (str): Name of the loaded file.\n        rows_loaded (int): Number of rows successfully inserted.\n    \"\"\"\n    cur.execute(\n        f\"\"\"\n        UPDATE {functions.METADATA_TABLE} \n        SET rows_loaded = %s, load_status = 'SUCCESS' \n        WHERE file_name = %s\n        \"\"\",\n        (rows_loaded, filename),\n    )\n    logger.debug(f\"\ud83d\ude80 Loading {functions.METADATA_TABLE}\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.backup_policy","title":"<code>snowflake_ingestion.backup_policy</code>","text":""},{"location":"docstrings/#snowflake_ingestion.backup_policy.create_and_set_backup","title":"<code>create_and_set_backup(cur)</code>","text":"<p>Creates the backup policies and backup sets for the data warehouse.</p> <p>Executes the SQL script to create the monthly backup policies and link them to the target objects (full database, raw table, final schema).</p> <p>Parameters:</p> Name Type Description Default <code>cur</code> <code>SnowflakeCursor</code> <p>Active Snowflake cursor.</p> required Source code in <code>snowflake_ingestion/backup_policy.py</code> <pre><code>def create_and_set_backup(cur: SnowflakeCursor) -&gt; None:\n    \"\"\"Creates the backup policies and backup sets for the data warehouse.\n\n    Executes the SQL script to create the monthly backup policies and\n    link them to the target objects (full database, raw table, final schema).\n\n    Args:\n        cur (snowflake.connector.cursor.SnowflakeCursor): Active Snowflake cursor.\n    \"\"\"\n    logger.info(\"\ud83d\udd10 Creating backup policies and sets...\")\n    sql_file = SQL_DIR / \"create_and_set_backup.sql\"\n    functions.run_sql_file(cur, sql_file)\n    logger.info(f\"\u2705 {functions.DW_NAME}_BACKUP retention : {functions.FULL_BACKUP_POLICY_DAYS}\")\n    logger.info(f\"\u2705 {functions.RAW_TABLE}_BACKUP retention : {functions.RAW_TABLE_BACKUP_POLICY_DAYS}\")\n    logger.info(f\"\u2705 {functions.FINAL_SCHEMA}_BACKUP retention : {functions.FINAL_SCHEMA_BACKUP_POLICY_DAYS}\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.backup_policy.main","title":"<code>main()</code>","text":"<p>Main initialization process for the Snowflake environment.</p> <p>Establishes connections with appropriate roles (SYSADMIN, SECURITYADMIN, ACCOUNTADMIN) and executes setup steps in order.</p> Source code in <code>snowflake_ingestion/backup_policy.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main initialization process for the Snowflake environment.\n\n    Establishes connections with appropriate roles (SYSADMIN, SECURITYADMIN, ACCOUNTADMIN)\n    and executes setup steps in order.\n    \"\"\"\n    try:\n        conn = functions.connect_with_role(functions.USER, functions.PASSWORD, functions.ACCOUNT, \"SYSADMIN\")\n        with conn.cursor() as cur:\n            create_and_set_backup(cur)\n        conn.close()\n\n\n        logger.info(\"\ud83c\udfaf Complete initialization finished successfully!\")\n    except Exception as e:\n        logger.error(e)\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_functions","title":"<code>snowflake_ingestion.tests.test_functions</code>","text":""},{"location":"docstrings/#snowflake_ingestion.tests.test_functions.test_connect_with_role_autocommit_enabled","title":"<code>test_connect_with_role_autocommit_enabled()</code>","text":"<p>Test unitaire v\u00e9rifiant que l'autocommit est toujours activ\u00e9. V\u00e9rifie que le param\u00e8tre autocommit=True est syst\u00e9matiquement pass\u00e9 \u00e0 la connexion Snowflake, ind\u00e9pendamment des autres param\u00e8tres.</p> Source code in <code>snowflake_ingestion/tests/test_functions.py</code> <pre><code>def test_connect_with_role_autocommit_enabled():\n    \"\"\"Test unitaire v\u00e9rifiant que l'autocommit est toujours activ\u00e9.\n    V\u00e9rifie que le param\u00e8tre autocommit=True est syst\u00e9matiquement pass\u00e9\n    \u00e0 la connexion Snowflake, ind\u00e9pendamment des autres param\u00e8tres.\n    \"\"\"\n    mock_connection = Mock()\n    with patch('snowflake_ingestion.functions.snowflake.connector.connect', return_value=mock_connection) as mock_connect:\n        connect_with_role(\"different_user\", \"different_pass\", \"different_account\", \"different_role\")\n        call_kwargs = mock_connect.call_args.kwargs\n        assert call_kwargs['autocommit'] == True\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_functions.test_connect_with_role_parameters_passed_correctly","title":"<code>test_connect_with_role_parameters_passed_correctly()</code>","text":"<p>Test unitaire v\u00e9rifiant la transmission correcte des param\u00e8tres. V\u00e9rifie que tous les param\u00e8tres (user, password, account, role) sont correctement transmis \u00e0 snowflake.connector.connect sans modification.</p> Source code in <code>snowflake_ingestion/tests/test_functions.py</code> <pre><code>def test_connect_with_role_parameters_passed_correctly():\n    \"\"\"Test unitaire v\u00e9rifiant la transmission correcte des param\u00e8tres.\n    V\u00e9rifie que tous les param\u00e8tres (user, password, account, role) sont\n    correctement transmis \u00e0 snowflake.connector.connect sans modification.\n    \"\"\"\n    mock_connection = Mock()\n    with patch('snowflake_ingestion.functions.snowflake.connector.connect', return_value=mock_connection) as mock_connect:\n        test_params = {\n            'user': 'test_user',\n            'password': 'test_password', \n            'account': 'test_account',\n            'role': 'test_role'\n        }\n        connect_with_role(**test_params)\n        call_kwargs = mock_connect.call_args.kwargs\n        for key, value in test_params.items():\n            assert call_kwargs[key] == value\n        assert call_kwargs['autocommit'] == True\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_functions.test_connect_with_role_success","title":"<code>test_connect_with_role_success()</code>","text":"<p>Test unitaire de connect_with_role en cas de succ\u00e8s. V\u00e9rifie que la fonction appelle snowflake.connector.connect avec les bons param\u00e8tres, active l'autocommit et retourne l'objet connexion.</p> Source code in <code>snowflake_ingestion/tests/test_functions.py</code> <pre><code>def test_connect_with_role_success():\n    \"\"\"Test unitaire de connect_with_role en cas de succ\u00e8s.\n    V\u00e9rifie que la fonction appelle snowflake.connector.connect avec les bons\n    param\u00e8tres, active l'autocommit et retourne l'objet connexion.\n    \"\"\"\n    mock_connection = Mock()\n    with patch('snowflake_ingestion.functions.snowflake.connector.connect', return_value=mock_connection) as mock_connect:\n        result = connect_with_role(\"user\", \"pass\", \"account\", \"role\")\n        mock_connect.assert_called_once_with(\n            user=\"user\",\n            password=\"pass\", \n            account=\"account\",\n            role=\"role\",\n            autocommit=True\n        )\n        assert result == mock_connection\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_functions.test_run_sql_file","title":"<code>test_run_sql_file()</code>","text":"<p>Test unitaire de la fonction run_sql_file avec substitution de variables. V\u00e9rifie que les placeholders dans le SQL sont correctement remplac\u00e9s par les valeurs des variables globales correspondantes.</p> Source code in <code>snowflake_ingestion/tests/test_functions.py</code> <pre><code>def test_run_sql_file():\n    \"\"\"Test unitaire de la fonction run_sql_file avec substitution de variables.\n    V\u00e9rifie que les placeholders dans le SQL sont correctement remplac\u00e9s\n    par les valeurs des variables globales correspondantes.\n    \"\"\"\n    mock_cursor = Mock()\n    sql_content = \"SELECT * FROM RAW_TABLE_PLACEHOLDER WHERE user = USER_PLACEHOLDER;\"\n    with patch('builtins.open', mock_open(read_data=sql_content)):\n        with patch('snowflake_ingestion.functions.RAW_TABLE', 'my_table'):\n            with patch('snowflake_ingestion.functions.USER', 'test_user'):\n                run_sql_file(mock_cursor, Path(\"test.sql\"))\n    mock_cursor.execute.assert_called_once_with(\"SELECT * FROM my_table WHERE user = test_user\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_functions.test_run_sql_file_multiple_statements","title":"<code>test_run_sql_file_multiple_statements()</code>","text":"<p>Test unitaire de run_sql_file avec multiples requ\u00eates. V\u00e9rifie que les requ\u00eates s\u00e9par\u00e9es par des points-virgules sont correctement divis\u00e9es et ex\u00e9cut\u00e9es individuellement.</p> Source code in <code>snowflake_ingestion/tests/test_functions.py</code> <pre><code>def test_run_sql_file_multiple_statements():\n    \"\"\"Test unitaire de run_sql_file avec multiples requ\u00eates.\n    V\u00e9rifie que les requ\u00eates s\u00e9par\u00e9es par des points-virgules sont\n    correctement divis\u00e9es et ex\u00e9cut\u00e9es individuellement.\n    \"\"\"\n    mock_cursor = Mock()\n    sql_content = \"SELECT 1; SELECT 2; SELECT 3;\"\n    with patch('builtins.open', mock_open(read_data=sql_content)):\n        run_sql_file(mock_cursor, Path(\"test.sql\"))\n    assert mock_cursor.execute.call_count == 3\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_functions.test_run_sql_file_variable_not_found","title":"<code>test_run_sql_file_variable_not_found()</code>","text":"<p>Test unitaire de run_sql_file avec variable non trouv\u00e9e. V\u00e9rifie que les placeholders sans variable globale correspondante sont remplac\u00e9s par la valeur par d\u00e9faut . Source code in <code>snowflake_ingestion/tests/test_functions.py</code> <pre><code>def test_run_sql_file_variable_not_found():\n    \"\"\"Test unitaire de run_sql_file avec variable non trouv\u00e9e.\n    V\u00e9rifie que les placeholders sans variable globale correspondante\n    sont remplac\u00e9s par la valeur par d\u00e9faut &lt;VAR_NOT_FOUND&gt;.\n    \"\"\"\n    mock_cursor = Mock()\n    sql_content = \"SELECT * FROM UNKNOWN_PLACEHOLDER;\"\n    with patch('builtins.open', mock_open(read_data=sql_content)):\n        run_sql_file(mock_cursor, Path(\"test.sql\"))\n    mock_cursor.execute.assert_called_once_with(\"SELECT * FROM &lt;UNKNOWN_NOT_FOUND&gt;\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_functions.test_use_context","title":"<code>test_use_context(mocker)</code>","text":"<p>Test unitaire de la fonction use_context. V\u00e9rifie que la fonction ex\u00e9cute les 3 commandes SQL attendues pour configurer le contexte Snowflake (warehouse, database, schema) dans le bon ordre.</p> <p>Parameters:</p> Name Type Description Default <code>mocker</code> <code>Mock</code> <p>Fixture pytest pour le mocking</p> required Source code in <code>snowflake_ingestion/tests/test_functions.py</code> <pre><code>def test_use_context(mocker: Mock):\n    \"\"\"Test unitaire de la fonction use_context.\n    V\u00e9rifie que la fonction ex\u00e9cute les 3 commandes SQL attendues pour configurer\n    le contexte Snowflake (warehouse, database, schema) dans le bon ordre.\n\n    Args:\n        mocker: Fixture pytest pour le mocking\n    \"\"\"\n    mock_cursor = Mock()\n    with patch.object(mock_cursor, 'execute') as mock_execute:\n        use_context(mock_cursor, \"WH\", \"DB\", \"TEST\")\n        assert mock_execute.call_count == 3\n        calls = [call[0][0] for call in mock_execute.call_args_list]\n        assert \"USE WAREHOUSE WH\" in calls\n        assert \"USE DATABASE DB\" in calls  \n        assert \"USE SCHEMA SCHEMA_TEST\" in calls\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_functions.test_use_context_exception","title":"<code>test_use_context_exception()</code>","text":"<p>Test unitaire de la gestion d'erreur de use_context. V\u00e9rifie que la fonction l\u00e8ve une exception SystemExit lorsqu'une erreur se produit lors de l'ex\u00e9cution des commandes SQL.</p> Source code in <code>snowflake_ingestion/tests/test_functions.py</code> <pre><code>def test_use_context_exception():\n    \"\"\"Test unitaire de la gestion d'erreur de use_context.\n    V\u00e9rifie que la fonction l\u00e8ve une exception SystemExit lorsqu'une erreur\n    se produit lors de l'ex\u00e9cution des commandes SQL.\n    \"\"\"\n    mock_cursor = Mock()\n    mock_cursor.execute.side_effect = Exception(\"DB error\")\n\n    with pytest.raises(SystemExit):\n        use_context(mock_cursor, \"WH\", \"DB\", \"SCHEMA\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_init_infra_snowflake","title":"<code>snowflake_ingestion.tests.test_init_infra_snowflake</code>","text":""},{"location":"docstrings/#snowflake_ingestion.tests.test_init_infra_snowflake.test_create_roles_and_user","title":"<code>test_create_roles_and_user()</code>","text":"<p>Unit test for the create_roles_and_user function. Tests the creation of Snowflake roles and users as part of infrastructure initialization. Verifies that the appropriate SQL script is executed and success/failure logs are properly recorded during the role and user creation process.</p> Source code in <code>snowflake_ingestion/tests/test_init_infra_snowflake.py</code> <pre><code>def test_create_roles_and_user():\n    \"\"\"\n    Unit test for the create_roles_and_user function.\n    Tests the creation of Snowflake roles and users as part of infrastructure initialization.\n    Verifies that the appropriate SQL script is executed and success/failure logs are properly\n    recorded during the role and user creation process.\n    \"\"\"\n    mock_cursor = Mock()\n    with patch('snowflake_ingestion.init_infra_snowflake.functions.run_sql_file') as mock_run_sql:\n        with patch('snowflake_ingestion.init_infra_snowflake.logger') as mock_logger:\n            infra.create_roles_and_user(mock_cursor)\n\n            mock_run_sql.assert_called_once_with(mock_cursor, infra.SQL_DIR / \"create_roles_and_user.sql\")\n            mock_logger.info.assert_any_call(\"\ud83d\udd10 Creating roles and users...\")\n            mock_logger.info.assert_any_call(\"\u2705 Roles and users created\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_init_infra_snowflake.test_grant_privileges","title":"<code>test_grant_privileges()</code>","text":"<p>Unit test for the grant_privileges function. Tests the granting of privileges to the created roles in Snowflake. Verifies that the correct SQL script is executed and appropriate log messages are recorded during the privilege granting process.</p> Source code in <code>snowflake_ingestion/tests/test_init_infra_snowflake.py</code> <pre><code>def test_grant_privileges():\n    \"\"\"\n    Unit test for the grant_privileges function.\n    Tests the granting of privileges to the created roles in Snowflake.\n    Verifies that the correct SQL script is executed and appropriate log messages are recorded\n    during the privilege granting process.\n    \"\"\"\n    mock_cursor = Mock()\n    with patch('snowflake_ingestion.init_infra_snowflake.functions.run_sql_file') as mock_run_sql:\n        with patch('snowflake_ingestion.init_infra_snowflake.logger') as mock_logger:\n            infra.grant_privileges(mock_cursor)\n\n            mock_run_sql.assert_called_once_with(mock_cursor, infra.SQL_DIR / \"grant_privileges.sql\")\n            mock_logger.info.assert_any_call(\"\ud83d\udd11 Granting privileges to the roles...\")\n            mock_logger.info.assert_any_call(\"\u2705 Privileges granted\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_init_infra_snowflake.test_main_exception","title":"<code>test_main_exception()</code>","text":"<p>Unit test for the main function when an exception occurs during initialization. Tests error handling by simulating a connection failure and verifying that the exception is properly caught and logged as an error.</p> Source code in <code>snowflake_ingestion/tests/test_init_infra_snowflake.py</code> <pre><code>def test_main_exception():\n    \"\"\"\n    Unit test for the main function when an exception occurs during initialization.\n    Tests error handling by simulating a connection failure and verifying that the\n    exception is properly caught and logged as an error.\n    \"\"\"\n    with patch('snowflake_ingestion.init_infra_snowflake.functions.connect_with_role', side_effect=Exception(\"Connection failed\")):\n        with patch('snowflake_ingestion.init_infra_snowflake.logger') as mock_logger:\n            infra.main()\n            mock_logger.error.assert_called_once()\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_init_infra_snowflake.test_main_success","title":"<code>test_main_success()</code>","text":"<p>Unit test for the main function when the infrastructure initialization completes successfully. Tests the complete initialization flow including warehouse setup, role creation, and privilege granting. Verifies that all three connections are made with the appropriate roles (ACCOUNTADMIN, SYSADMIN, SECURITYADMIN) and that all initialization functions are called in sequence with successful logging.</p> Source code in <code>snowflake_ingestion/tests/test_init_infra_snowflake.py</code> <pre><code>def test_main_success():\n    \"\"\"\n    Unit test for the main function when the infrastructure initialization completes successfully.\n    Tests the complete initialization flow including warehouse setup, role creation,\n    and privilege granting. Verifies that all three connections are made with the\n    appropriate roles (ACCOUNTADMIN, SYSADMIN, SECURITYADMIN) and that all\n    initialization functions are called in sequence with successful logging.\n    \"\"\"\n    mock_conns = [Mock(), Mock(), Mock()]\n    for mock_conn in mock_conns:\n        mock_conn.cursor.return_value.__enter__ = Mock(return_value=Mock())\n        mock_conn.cursor.return_value.__exit__ = Mock(return_value=None)\n\n    call_counter = 0\n    def connect_side_effect(*args, **kwargs):\n        nonlocal call_counter\n        if call_counter &lt; len(mock_conns):\n            result = mock_conns[call_counter]\n            call_counter += 1\n            return result\n        return Mock()\n\n    with patch('snowflake_ingestion.init_infra_snowflake.functions.RETENTION_TIME', 90):\n        with patch('snowflake_ingestion.init_infra_snowflake.functions.connect_with_role', \n                   side_effect=connect_side_effect) as mock_connect:\n            with patch('snowflake_ingestion.init_infra_snowflake.setup_data_warehouse') as mock_setup:\n                with patch('snowflake_ingestion.init_infra_snowflake.create_roles_and_user') as mock_create:\n                    with patch('snowflake_ingestion.init_infra_snowflake.grant_privileges') as mock_grant:\n                        with patch('snowflake_ingestion.init_infra_snowflake.logger') as mock_logger:\n                            infra.main()\n\n                            assert mock_connect.call_count == 3\n                            mock_connect.assert_any_call(infra.functions.USER, infra.functions.PASSWORD, infra.functions.ACCOUNT, 'ACCOUNTADMIN')\n                            mock_connect.assert_any_call(infra.functions.USER, infra.functions.PASSWORD, infra.functions.ACCOUNT, 'SYSADMIN')\n                            mock_connect.assert_any_call(infra.functions.USER, infra.functions.PASSWORD, infra.functions.ACCOUNT, 'SECURITYADMIN')\n\n                            mock_setup.assert_called_once()\n                            mock_create.assert_called_once()\n                            mock_grant.assert_called_once()\n                            mock_logger.info.assert_called_with(\"\ud83c\udfaf Complete initialization finished successfully!\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_init_infra_snowflake.test_set_data_retention","title":"<code>test_set_data_retention()</code>","text":"<p>Unit test for the set_data_retention function. Verifies that the function executes the correct SQL file with the specified retention time and logs appropriate messages when setting data retention policies.</p> Source code in <code>snowflake_ingestion/tests/test_init_infra_snowflake.py</code> <pre><code>def test_set_data_retention():\n    \"\"\"\n    Unit test for the set_data_retention function.\n    Verifies that the function executes the correct SQL file with the specified retention time\n    and logs appropriate messages when setting data retention policies.\n    \"\"\"\n    mock_cursor = Mock()\n\n    with patch('snowflake_ingestion.init_infra_snowflake.functions.RETENTION_TIME', 90):\n        with patch('snowflake_ingestion.init_infra_snowflake.functions.run_sql_file') as mock_run_sql:\n            with patch('snowflake_ingestion.init_infra_snowflake.logger') as mock_logger:\n                infra.set_data_retention(mock_cursor)\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_init_infra_snowflake.test_setup_data_warehouse","title":"<code>test_setup_data_warehouse()</code>","text":"<p>Unit test for the setup_data_warehouse function. Tests the creation of warehouse, database, and schemas infrastructure in Snowflake. Verifies that the correct SQL script is executed and appropriate log messages are recorded during the warehouse setup process.</p> Source code in <code>snowflake_ingestion/tests/test_init_infra_snowflake.py</code> <pre><code>def test_setup_data_warehouse():\n    \"\"\"\n    Unit test for the setup_data_warehouse function.\n    Tests the creation of warehouse, database, and schemas infrastructure in Snowflake.\n    Verifies that the correct SQL script is executed and appropriate log messages are recorded\n    during the warehouse setup process.\n    \"\"\"\n    mock_cursor = Mock()\n    with patch('snowflake_ingestion.init_infra_snowflake.functions.run_sql_file') as mock_run_sql:\n        with patch('snowflake_ingestion.init_infra_snowflake.logger') as mock_logger:\n            infra.setup_data_warehouse(mock_cursor)\n\n            mock_run_sql.assert_called_once_with(mock_cursor, infra.SQL_DIR / \"setup_data_warehouse.sql\")\n            mock_logger.info.assert_any_call(\"\ud83c\udfd7\ufe0f  Creating warehouse, database and schemas...\")\n            mock_logger.info.assert_any_call(\"\u2705 Warehouse and schemas created\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_scrape_links","title":"<code>snowflake_ingestion.tests.test_scrape_links</code>","text":""},{"location":"docstrings/#snowflake_ingestion.tests.test_scrape_links.test_get_parquet_links_success","title":"<code>test_get_parquet_links_success()</code>","text":"<p>Test successful extraction of parquet file links from the NYC TLC website HTML content. Verifies that only links with title 'Yellow Taxi Trip Records' are extracted and returned.</p> Source code in <code>snowflake_ingestion/tests/test_scrape_links.py</code> <pre><code>def test_get_parquet_links_success():\n    \"\"\"\n    Test successful extraction of parquet file links from the NYC TLC website HTML content.\n    Verifies that only links with title 'Yellow Taxi Trip Records' are extracted and returned.\n    \"\"\"\n    mock_html_content = \"\"\"\n    &lt;html&gt;\n        &lt;a title=\"Yellow Taxi Trip Records\" href=\"https://example.com/file1.parquet\"&gt;Link1&lt;/a&gt;\n        &lt;a title=\"Yellow Taxi Trip Records\" href=\"https://example.com/file2.parquet\"&gt;Link2&lt;/a&gt;\n        &lt;a title=\"Other Title\" href=\"https://example.com/file3.parquet\"&gt;Link3&lt;/a&gt;\n    &lt;/html&gt;\n    \"\"\"\n    mock_response = Mock()\n    mock_response.content = mock_html_content.encode()\n\n    mock_tree = Mock()\n    mock_link1 = Mock()\n    mock_link1.get.return_value = \"https://example.com/file1.parquet\"\n    mock_link2 = Mock()\n    mock_link2.get.return_value = \"https://example.com/file2.parquet\"\n    mock_tree.xpath.return_value = [mock_link1, mock_link2]\n\n    with patch('snowflake_ingestion.scrape_links.requests.get') as mock_get:\n        with patch('snowflake_ingestion.scrape_links.html.fromstring') as mock_fromstring:\n            with patch('snowflake_ingestion.scrape_links.get_xpath', return_value=\"//a[@title='Yellow Taxi Trip Records']\"):\n                with patch('snowflake_ingestion.scrape_links.logger') as mock_logger:\n                    mock_get.return_value = mock_response\n                    mock_fromstring.return_value = mock_tree\n\n                    result = scrape.get_parquet_links()\n\n                    mock_get.assert_called_once_with(\"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\")\n                    mock_logger.info.assert_called_with(\"\ud83c\udf10 Starting NYC Taxi data scraping\")\n                    assert result == [\"https://example.com/file1.parquet\", \"https://example.com/file2.parquet\"]\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_scrape_links.test_get_scraping_year_with_empty_env_early_month","title":"<code>test_get_scraping_year_with_empty_env_early_month()</code>","text":"<p>Test get_scraping_year behavior when SCRAPING_YEAR is empty and current month is early (January to March). The function should default to the previous year since current year's data may not be fully available yet.</p> Source code in <code>snowflake_ingestion/tests/test_scrape_links.py</code> <pre><code>def test_get_scraping_year_with_empty_env_early_month():\n    \"\"\"\n    Test get_scraping_year behavior when SCRAPING_YEAR is empty and current month is early (January to March).\n    The function should default to the previous year since current year's data may not be fully available yet.\n    \"\"\"\n    with patch('snowflake_ingestion.scrape_links.functions.SCRAPING_YEAR', ''):\n        with patch('snowflake_ingestion.scrape_links.current_month', 1):\n            result = scrape.get_scraping_year()\n            expected = scrape.current_year - 1\n            assert result == expected\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_scrape_links.test_get_scraping_year_with_empty_env_late_month","title":"<code>test_get_scraping_year_with_empty_env_late_month()</code>","text":"<p>Test get_scraping_year behavior when SCRAPING_YEAR is empty and current month is late (April to December). The function should default to the current year for scraping operations.</p> Source code in <code>snowflake_ingestion/tests/test_scrape_links.py</code> <pre><code>def test_get_scraping_year_with_empty_env_late_month():\n    \"\"\"\n    Test get_scraping_year behavior when SCRAPING_YEAR is empty and current month is late (April to December).\n    The function should default to the current year for scraping operations.\n    \"\"\"\n    with patch('snowflake_ingestion.scrape_links.functions.SCRAPING_YEAR', ''):\n        with patch('snowflake_ingestion.scrape_links.current_month', 4):\n            result = scrape.get_scraping_year()\n            expected = scrape.current_year\n            assert result == expected\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_scrape_links.test_get_scraping_year_with_invalid_env_early_month","title":"<code>test_get_scraping_year_with_invalid_env_early_month()</code>","text":"<p>Test get_scraping_year behavior when SCRAPING_YEAR contains invalid non-numeric data in early months. The function should log an error and default to the previous year while handling the invalid input gracefully.</p> Source code in <code>snowflake_ingestion/tests/test_scrape_links.py</code> <pre><code>def test_get_scraping_year_with_invalid_env_early_month():\n    \"\"\"\n    Test get_scraping_year behavior when SCRAPING_YEAR contains invalid non-numeric data in early months.\n    The function should log an error and default to the previous year while handling the invalid input gracefully.\n    \"\"\"\n    with patch('snowflake_ingestion.scrape_links.functions.SCRAPING_YEAR', 'invalid'):\n        with patch('snowflake_ingestion.scrape_links.current_month', 3):\n            with patch('snowflake_ingestion.scrape_links.logger') as mock_logger:\n                result = scrape.get_scraping_year()\n                expected = scrape.current_year - 1\n                assert result == expected\n                mock_logger.error.assert_called_once()\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_scrape_links.test_get_scraping_year_with_invalid_env_late_month","title":"<code>test_get_scraping_year_with_invalid_env_late_month()</code>","text":"<p>Test get_scraping_year behavior when SCRAPING_YEAR contains invalid non-numeric data in late months. The function should log an error and default to the current year while handling the invalid input gracefully.</p> Source code in <code>snowflake_ingestion/tests/test_scrape_links.py</code> <pre><code>def test_get_scraping_year_with_invalid_env_late_month():\n    \"\"\"\n    Test get_scraping_year behavior when SCRAPING_YEAR contains invalid non-numeric data in late months.\n    The function should log an error and default to the current year while handling the invalid input gracefully.\n    \"\"\"\n    with patch('snowflake_ingestion.scrape_links.functions.SCRAPING_YEAR', 'invalid'):\n        with patch('snowflake_ingestion.scrape_links.current_month', 12):\n            with patch('snowflake_ingestion.scrape_links.logger') as mock_logger:\n                result = scrape.get_scraping_year()\n                expected = scrape.current_year\n                assert result == expected\n                mock_logger.error.assert_called_once()\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_scrape_links.test_get_scraping_year_with_valid_env","title":"<code>test_get_scraping_year_with_valid_env()</code>","text":"<p>Test that get_scraping_year correctly parses and returns a valid integer value from the SCRAPING_YEAR environment variable. Verifies that when SCRAPING_YEAR is set to '2023', the function returns the integer 2023.</p> Source code in <code>snowflake_ingestion/tests/test_scrape_links.py</code> <pre><code>def test_get_scraping_year_with_valid_env():\n    \"\"\"\n    Test that get_scraping_year correctly parses and returns a valid integer value from the SCRAPING_YEAR environment variable.\n    Verifies that when SCRAPING_YEAR is set to '2023', the function returns the integer 2023.\n    \"\"\"\n    with patch('snowflake_ingestion.scrape_links.functions.SCRAPING_YEAR', '2023'):\n        result = scrape.get_scraping_year()\n        assert result == 2023\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_scrape_links.test_get_xpath","title":"<code>test_get_xpath()</code>","text":"<p>Test that get_xpath generates the correct XPath expression for locating Yellow Taxi Trip Records links. The XPath should include both the scraping year and current year to capture relevant data files.</p> Source code in <code>snowflake_ingestion/tests/test_scrape_links.py</code> <pre><code>def test_get_xpath():\n    \"\"\"\n    Test that get_xpath generates the correct XPath expression for locating Yellow Taxi Trip Records links.\n    The XPath should include both the scraping year and current year to capture relevant data files.\n    \"\"\"\n    with patch('snowflake_ingestion.scrape_links.get_scraping_year', return_value=2023):\n        with patch('snowflake_ingestion.scrape_links.current_year', 2024):\n            result = scrape.get_xpath()\n            expected = \"//a[@title='Yellow Taxi Trip Records' and (contains(@href, '2023') or contains(@href, '2024'))]\"\n            assert result == expected\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_scrape_links.test_main_file_parsing","title":"<code>test_main_file_parsing()</code>","text":"<p>Test that the main function correctly parses filename patterns to extract year and month components. Verifies that URLs like 'yellow_tripdata_2023-07.parquet' are correctly parsed into (2023, 7) tuples.</p> Source code in <code>snowflake_ingestion/tests/test_scrape_links.py</code> <pre><code>def test_main_file_parsing():\n    \"\"\"\n    Test that the main function correctly parses filename patterns to extract year and month components.\n    Verifies that URLs like 'yellow_tripdata_2023-07.parquet' are correctly parsed into (2023, 7) tuples.\n    \"\"\"\n    mock_conn = MagicMock()\n    mock_cursor = MagicMock()\n    mock_conn.cursor.return_value.__enter__.return_value = mock_cursor\n    mock_cursor.fetchone.return_value = None\n\n    with patch('snowflake_ingestion.scrape_links.functions.connect_with_role', return_value=mock_conn):\n        with patch('snowflake_ingestion.scrape_links.functions.use_context'):\n            with patch('snowflake_ingestion.scrape_links.setup_meta_table'):\n                with patch('snowflake_ingestion.scrape_links.get_parquet_links') as mock_links:\n                    with patch('snowflake_ingestion.scrape_links.functions.run_sql_file'):\n                        with patch('snowflake_ingestion.scrape_links.logger'):\n                            mock_links.return_value = [\"https://example.com/yellow_tripdata_2023-07.parquet\"]\n                            scrape.main()\n\n                            insert_call = None\n                            for call in mock_cursor.execute.call_args_list:\n                                if 'INSERT' in str(call[0][0]):\n                                    insert_call = call\n                                    break\n\n                            assert insert_call is not None\n                            assert insert_call[0][1] == (\"https://example.com/yellow_tripdata_2023-07.parquet\", \n                                                        \"yellow_tripdata_2023-07.parquet\", 2023, 7)\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_scrape_links.test_main_with_new_files","title":"<code>test_main_with_new_files()</code>","text":"<p>Test the main scraping workflow when new parquet files are detected that don't exist in the metadata table. Verifies that new files trigger INSERT operations into the metadata table with appropriate logging.</p> Source code in <code>snowflake_ingestion/tests/test_scrape_links.py</code> <pre><code>def test_main_with_new_files():\n    \"\"\"\n    Test the main scraping workflow when new parquet files are detected that don't exist in the metadata table.\n    Verifies that new files trigger INSERT operations into the metadata table with appropriate logging.\n    \"\"\"\n    mock_conn = MagicMock()\n    mock_cursor = MagicMock()\n    mock_conn.cursor.return_value.__enter__.return_value = mock_cursor\n    mock_cursor.fetchone.side_effect = [None, None, [5]]\n\n    with patch('snowflake_ingestion.scrape_links.functions.connect_with_role', return_value=mock_conn):\n        with patch('snowflake_ingestion.scrape_links.functions.use_context'):\n            with patch('snowflake_ingestion.scrape_links.setup_meta_table'):\n                with patch('snowflake_ingestion.scrape_links.get_parquet_links') as mock_links:\n                    with patch('snowflake_ingestion.scrape_links.functions.run_sql_file'):\n                        with patch('snowflake_ingestion.scrape_links.logger') as mock_logger:\n\n                            mock_links.return_value = [\n                                \"https://example.com/yellow_tripdata_2023-01.parquet\",\n                                \"https://example.com/yellow_tripdata_2023-02.parquet\"\n                            ]\n\n                            scrape.main()\n\n                            assert mock_cursor.execute.call_count &gt;= 4\n                            mock_logger.info.assert_any_call(\"\ud83d\udcce 2 links found\")\n                            mock_logger.info.assert_any_call(\"\u2795 New file detected : yellow_tripdata_2023-01.parquet\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_scrape_links.test_main_without_new_files","title":"<code>test_main_without_new_files()</code>","text":"<p>Test the main scraping workflow when all discovered parquet files already exist in the metadata table. Verifies that no INSERT operations occur and appropriate informational and warning logs are recorded.</p> Source code in <code>snowflake_ingestion/tests/test_scrape_links.py</code> <pre><code>def test_main_without_new_files():\n    \"\"\"\n    Test the main scraping workflow when all discovered parquet files already exist in the metadata table.\n    Verifies that no INSERT operations occur and appropriate informational and warning logs are recorded.\n    \"\"\"\n    mock_conn = MagicMock()\n    mock_cursor = MagicMock()\n    mock_conn.cursor.return_value.__enter__.return_value = mock_cursor\n    mock_cursor.fetchone.side_effect = [[1], [0]]\n\n    with patch('snowflake_ingestion.scrape_links.functions.connect_with_role', return_value=mock_conn):\n        with patch('snowflake_ingestion.scrape_links.functions.use_context'):\n            with patch('snowflake_ingestion.scrape_links.setup_meta_table'):\n                with patch('snowflake_ingestion.scrape_links.get_parquet_links') as mock_links:\n                    with patch('snowflake_ingestion.scrape_links.functions.run_sql_file'):\n                        with patch('snowflake_ingestion.scrape_links.logger') as mock_logger:\n                            mock_links.return_value = [\"https://example.com/yellow_tripdata_2023-01.parquet\"]\n                            scrape.main()\n                            mock_logger.info.assert_any_call(\"\ud83d\udcce 1 link found\")\n                            mock_logger.info.assert_any_call(\"\u23ed\ufe0f  yellow_tripdata_2023-01.parquet already referenced\")\n                            mock_logger.info.assert_any_call(\"\u2705 Scraping completed\")\n                            mock_logger.warning.assert_called_with(\"\u26a0\ufe0f  No new files to load.\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_scrape_links.test_setup_meta_table","title":"<code>test_setup_meta_table()</code>","text":"<p>Test that setup_meta_table correctly executes the SQL script for creating or verifying the metadata table. Verifies that the appropriate SQL file is executed and success/failure logs are recorded.</p> Source code in <code>snowflake_ingestion/tests/test_scrape_links.py</code> <pre><code>def test_setup_meta_table():\n    \"\"\"\n    Test that setup_meta_table correctly executes the SQL script for creating or verifying the metadata table.\n    Verifies that the appropriate SQL file is executed and success/failure logs are recorded.\n    \"\"\"\n    mock_cursor = Mock()\n    with patch('snowflake_ingestion.scrape_links.functions.run_sql_file') as mock_run_sql:\n        with patch('snowflake_ingestion.scrape_links.logger') as mock_logger:\n            scrape.setup_meta_table(mock_cursor)\n            mock_run_sql.assert_called_once_with(mock_cursor, scrape.SQL_DIR / \"setup_meta_table.sql\")\n            mock_logger.info.assert_any_call(\"\ud83d\udccb Verification/Creation of metadata table\")\n            mock_logger.info.assert_any_call(\"\u2705 Metadata table ready\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_upload_stage","title":"<code>snowflake_ingestion.tests.test_upload_stage</code>","text":""},{"location":"docstrings/#snowflake_ingestion.tests.test_upload_stage.test_download_and_upload_file_http_error","title":"<code>test_download_and_upload_file_http_error()</code>","text":"<p>Unit test for download_and_upload_file in case of HTTP error. Verifies that the function raises an exception when the HTTP download fails.</p> Source code in <code>snowflake_ingestion/tests/test_upload_stage.py</code> <pre><code>def test_download_and_upload_file_http_error():\n    \"\"\"Unit test for download_and_upload_file in case of HTTP error.\n    Verifies that the function raises an exception when the HTTP download fails.\n    \"\"\"\n    mock_cursor = Mock()\n    mock_response = Mock()\n    mock_response.raise_for_status.side_effect = requests.HTTPError(\"HTTP Error\")\n\n    with patch('snowflake_ingestion.upload_stage.requests.get', return_value=mock_response):\n        with patch('snowflake_ingestion.upload_stage.tempfile.NamedTemporaryFile'):\n            with patch('snowflake_ingestion.upload_stage.logger'):\n                with pytest.raises(requests.HTTPError):\n                    stage.download_and_upload_file(mock_cursor, \"http://example.com/test.parquet\", \"test.parquet\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_upload_stage.test_download_and_upload_file_snowflake_error","title":"<code>test_download_and_upload_file_snowflake_error()</code>","text":"<p>Unit test for download_and_upload_file in case of Snowflake error. Verifies that the function raises an exception when the Snowflake upload fails.</p> Source code in <code>snowflake_ingestion/tests/test_upload_stage.py</code> <pre><code>def test_download_and_upload_file_snowflake_error():\n    \"\"\"Unit test for download_and_upload_file in case of Snowflake error.\n    Verifies that the function raises an exception when the Snowflake upload fails.\n    \"\"\"\n    mock_cursor = Mock()\n    mock_response = Mock()\n    mock_response.content = b\"fake parquet content\"\n    mock_response.raise_for_status = Mock()\n    mock_temp_file = Mock()\n    mock_temp_file.name = \"/tmp/tempfile_123.parquet\"\n    mock_temp_file.write = Mock()\n    mock_temp_file.flush = Mock()\n    mock_temp_file.__enter__ = Mock(return_value=mock_temp_file)\n    mock_temp_file.__exit__ = Mock(return_value=None)\n\n    with patch('snowflake_ingestion.upload_stage.requests.get', return_value=mock_response):\n        with patch('snowflake_ingestion.upload_stage.tempfile.NamedTemporaryFile', return_value=mock_temp_file):\n            with patch('snowflake_ingestion.upload_stage.logger'):\n                mock_cursor.execute.side_effect = Exception(\"Snowflake PUT failed\")       \n                with pytest.raises(Exception, match=\"Snowflake PUT failed\"):\n                    stage.download_and_upload_file(mock_cursor, \"http://example.com/test.parquet\", \"test.parquet\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_upload_stage.test_download_and_upload_file_success","title":"<code>test_download_and_upload_file_success()</code>","text":"<p>Unit test for download_and_upload_file in case of success. Verifies that the function downloads the file from the URL, uploads it to Snowflake via PUT, and automatically cleans up the temporary file.</p> Source code in <code>snowflake_ingestion/tests/test_upload_stage.py</code> <pre><code>def test_download_and_upload_file_success():\n    \"\"\"Unit test for download_and_upload_file in case of success.\n    Verifies that the function downloads the file from the URL, uploads it to Snowflake\n    via PUT, and automatically cleans up the temporary file.\n    \"\"\"\n    mock_cursor = Mock()\n    mock_response = Mock()\n    mock_response.content = b\"fake parquet content\"\n    mock_response.raise_for_status = Mock()\n    mock_temp_file = Mock()\n    mock_temp_file.name = \"/tmp/tempfile_123.parquet\"\n    mock_temp_file.write = Mock()\n    mock_temp_file.flush = Mock()\n    mock_temp_file.__enter__ = Mock(return_value=mock_temp_file)\n    mock_temp_file.__exit__ = Mock(return_value=None)\n\n    with patch('snowflake_ingestion.upload_stage.requests.get', return_value=mock_response):\n        with patch('snowflake_ingestion.upload_stage.tempfile.NamedTemporaryFile') as mock_tempfile:\n            with patch('snowflake_ingestion.upload_stage.logger') as mock_logger:\n                mock_tempfile.return_value = mock_temp_file\n                stage.download_and_upload_file(mock_cursor, \"http://example.com/test.parquet\", \"test.parquet\")\n\n                mock_response.raise_for_status.assert_called_once()\n                mock_temp_file.write.assert_called_once_with(b\"fake parquet content\")\n                mock_temp_file.flush.assert_called_once()\n                mock_cursor.execute.assert_called_once_with(\"PUT 'file:///tmp/tempfile_123.parquet' @~/test.parquet AUTO_COMPRESS=FALSE\")\n                mock_logger.info.assert_any_call(\"\ud83d\udce5 Downloading test.parquet...\")\n                mock_logger.info.assert_any_call(\"\ud83d\udce4 Uploading to Snowflake...\")\n                mock_logger.info.assert_any_call(\"\u2705 test.parquet uploaded and temporary file cleaned\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_upload_stage.test_download_and_upload_file_tempfile_error","title":"<code>test_download_and_upload_file_tempfile_error()</code>","text":"<p>Unit test for download_and_upload_file in case of temporary file creation error. Verifies that the function raises an exception when temporary file creation fails.</p> Source code in <code>snowflake_ingestion/tests/test_upload_stage.py</code> <pre><code>def test_download_and_upload_file_tempfile_error():\n    \"\"\"Unit test for download_and_upload_file in case of temporary file creation error.\n    Verifies that the function raises an exception when temporary file creation fails.\n    \"\"\"\n    mock_cursor = Mock()\n    mock_response = Mock()\n    mock_response.content = b\"fake parquet content\"\n    mock_response.raise_for_status = Mock()\n\n    with patch('snowflake_ingestion.upload_stage.requests.get', return_value=mock_response):\n        with patch('snowflake_ingestion.upload_stage.tempfile.NamedTemporaryFile') as mock_tempfile:\n            with patch('snowflake_ingestion.upload_stage.logger'):\n                mock_tempfile.side_effect = OSError(\"Cannot create temp file\")\n                with pytest.raises(OSError, match=\"Cannot create temp file\"):\n                    stage.download_and_upload_file(mock_cursor, \"http://example.com/test.parquet\", \"test.parquet\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_upload_stage.test_main_file_processing_flow","title":"<code>test_main_file_processing_flow()</code>","text":"<p>Unit test for the complete file processing flow. Verifies the order of operations: DB connection, metadata retrieval, download, upload, status update.</p> Source code in <code>snowflake_ingestion/tests/test_upload_stage.py</code> <pre><code>def test_main_file_processing_flow():\n    \"\"\"Unit test for the complete file processing flow.\n    Verifies the order of operations: DB connection, metadata retrieval,\n    download, upload, status update.\n    \"\"\"\n    mock_conn = MagicMock()\n    mock_cursor = MagicMock()\n    mock_conn.cursor.return_value.__enter__.return_value = mock_cursor\n\n    mock_cursor.fetchall.return_value = [(\"http://example.com/test.parquet\", \"test.parquet\")]\n    execute_calls = []\n\n    def track_execute(*args, **kwargs):\n        execute_calls.append((args, kwargs))\n        return MagicMock()\n\n    mock_cursor.execute.side_effect = track_execute\n\n    with patch('snowflake_ingestion.upload_stage.functions.connect_with_role', return_value=mock_conn):\n        with patch('snowflake_ingestion.upload_stage.functions.use_context'):\n            with patch('snowflake_ingestion.upload_stage.functions.run_sql_file'):\n                with patch('snowflake_ingestion.upload_stage.download_and_upload_file'):\n                    with patch('snowflake_ingestion.upload_stage.logger'):\n                        stage.main()\n                        staged_updates = []\n                        for args, kwargs in execute_calls:\n                            if len(args) &gt; 0 and 'UPDATE' in args[0] and 'STAGED' in args[0]:\n                                staged_updates.append((args, kwargs))\n                        assert len(staged_updates) == 1\n                        update_args = staged_updates[0][0]\n                        assert len(update_args) &gt;= 2\n                        assert update_args[1] == ('test.parquet',)\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_upload_stage.test_main_with_files","title":"<code>test_main_with_files()</code>","text":"<p>Unit test for the main function with files to upload. Verifies that the function retrieves the scraped files, downloads them, uploads them to Snowflake and updates the status in the metadata.</p> Source code in <code>snowflake_ingestion/tests/test_upload_stage.py</code> <pre><code>def test_main_with_files():\n    \"\"\"Unit test for the main function with files to upload.\n    Verifies that the function retrieves the scraped files, downloads them,\n    uploads them to Snowflake and updates the status in the metadata.\n    \"\"\"\n    mock_conn = MagicMock()\n    mock_cursor = MagicMock()\n    mock_conn.cursor.return_value.__enter__.return_value = mock_cursor\n\n    mock_cursor.fetchall.return_value = [\n        (\"http://example.com/file1.parquet\", \"file1.parquet\"),\n        (\"http://example.com/file2.parquet\", \"file2.parquet\")\n    ]\n\n    with patch('snowflake_ingestion.upload_stage.functions.connect_with_role', return_value=mock_conn):\n        with patch('snowflake_ingestion.upload_stage.functions.use_context'):\n            with patch('snowflake_ingestion.upload_stage.functions.run_sql_file'):\n                with patch('snowflake_ingestion.upload_stage.download_and_upload_file') as mock_download:\n                    with patch('snowflake_ingestion.upload_stage.logger') as mock_logger:\n                        stage.main()\n                        mock_logger.info.assert_any_call(\"\ud83d\udce6 2 files to upload\")\n                        mock_logger.info.assert_any_call(\"\u2705 file1.parquet uploaded\")\n                        mock_logger.info.assert_any_call(\"\u2705 file2.parquet uploaded\")\n                        update_calls = [call for call in mock_cursor.execute.call_args_list \n                                      if 'UPDATE' in str(call[0][0]) and 'STAGED' in str(call[0][0])]\n                        assert len(update_calls) == 2\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_upload_stage.test_main_with_upload_error","title":"<code>test_main_with_upload_error()</code>","text":"<p>Unit test for the main function with upload error. Verifies that the function correctly handles upload errors by updating the status to FAILED_STAGE.</p> Source code in <code>snowflake_ingestion/tests/test_upload_stage.py</code> <pre><code>def test_main_with_upload_error():\n    \"\"\"Unit test for the main function with upload error.\n    Verifies that the function correctly handles upload errors by updating\n    the status to FAILED_STAGE.\n    \"\"\"\n    mock_conn = MagicMock()\n    mock_cursor = MagicMock()\n    mock_conn.cursor.return_value.__enter__.return_value = mock_cursor\n    mock_cursor.fetchall.return_value = [(\"http://example.com/file1.parquet\", \"file1.parquet\")]\n\n    with patch('snowflake_ingestion.upload_stage.functions.connect_with_role', return_value=mock_conn):\n        with patch('snowflake_ingestion.upload_stage.functions.use_context'):\n            with patch('snowflake_ingestion.upload_stage.functions.run_sql_file'):\n                with patch('snowflake_ingestion.upload_stage.download_and_upload_file') as mock_download:\n                    with patch('snowflake_ingestion.upload_stage.logger') as mock_logger:\n                        mock_download.side_effect = Exception(\"Upload failed\")\n                        stage.main()\n                        mock_logger.error.assert_called_with(\"\u274c Upload error file1.parquet: Upload failed\")\n                        update_calls = [call for call in mock_cursor.execute.call_args_list \n                                      if 'FAILED_STAGE' in str(call[0][0])]\n                        assert len(update_calls) == 1\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table","title":"<code>snowflake_ingestion.tests.test_load_to_table</code>","text":""},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_cleanup_stage_file","title":"<code>test_cleanup_stage_file()</code>","text":"<p>Tests the removal of a processed file from the Snowflake stage. Verifies the correct REMOVE command is executed and a success log is recorded.</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_cleanup_stage_file():\n    \"\"\"Tests the removal of a processed file from the Snowflake stage.\n    Verifies the correct REMOVE command is executed and a success log is recorded.\n    \"\"\"\n    mock_cursor = Mock()\n    # Correction: Patcher le logger dans le module load_to_table directement\n    with patch('snowflake_ingestion.load_to_table.logger') as mock_logger:\n        load.cleanup_stage_file(mock_cursor, \"test_file.parquet\")\n        mock_cursor.execute.assert_called_once_with(\"REMOVE @~/test_file.parquet\")\n        mock_logger.info.assert_called_with(\"\u2705 test_file.parquet removed from stage\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_copy_file_to_table_and_count_copy_error","title":"<code>test_copy_file_to_table_and_count_copy_error()</code>","text":"<p>Tests the handling of an exception raised during the COPY INTO execution. Verifies that the exception is propagated and not caught within the function.</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_copy_file_to_table_and_count_copy_error():\n    \"\"\"Tests the handling of an exception raised during the COPY INTO execution.\n    Verifies that the exception is propagated and not caught within the function.\n    \"\"\"\n    mock_cursor = Mock()\n    mock_cursor.execute.side_effect = Exception(\"COPY failed\")\n\n    with patch('snowflake_ingestion.functions.run_sql_file'):\n        # Correction: Patcher le logger dans le module load_to_table directement\n        with patch('snowflake_ingestion.load_to_table.logger') as mock_logger:\n            table_schema = [(\"vendorid\", \"NUMBER\"), (\"tpep_pickup_datetime\", \"TIMESTAMP_NTZ\")]\n            with pytest.raises(Exception, match=\"COPY failed\"):\n                load.copy_file_to_table_and_count(mock_cursor, \"test_file.parquet\", table_schema)\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_copy_file_to_table_and_count_success","title":"<code>test_copy_file_to_table_and_count_success()</code>","text":"<p>Tests the successful execution of COPY INTO command. Verifies the command execution, correct parsing of the result, logging of success with row count, and the return of the correct number of loaded rows.</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_copy_file_to_table_and_count_success():\n    \"\"\"Tests the successful execution of COPY INTO command.\n    Verifies the command execution, correct parsing of the result,\n    logging of success with row count, and the return of the correct number of loaded rows.\n    \"\"\"\n    mock_cursor = Mock()\n    mock_cursor.fetchone.return_value = ('test_file.parquet', 'LOADED', 250, 250, 1, 0, None, None, None, None)\n\n    with patch('snowflake_ingestion.functions.run_sql_file'):\n        # Correction: Patcher le logger dans le module load_to_table directement\n        with patch('snowflake_ingestion.load_to_table.logger') as mock_logger:\n            table_schema = [(\"vendorid\", \"NUMBER\"), (\"tpep_pickup_datetime\", \"TIMESTAMP_NTZ\")]\n            result = load.copy_file_to_table_and_count(mock_cursor, \"test_file.parquet\", table_schema)\n            assert result == 250\n            mock_cursor.execute.assert_called_once()\n            mock_logger.info.assert_called_with(\"\u2705 test_file.parquet loaded (250 rows)\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_copy_file_to_table_and_count_zero_loaded","title":"<code>test_copy_file_to_table_and_count_zero_loaded()</code>","text":"<p>Tests the COPY INTO command when no rows are processed. Verifies the function returns 0 when the execution result indicates no files were processed.</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_copy_file_to_table_and_count_zero_loaded():\n    \"\"\"Tests the COPY INTO command when no rows are processed.\n    Verifies the function returns 0 when the execution result indicates no files were processed.\n    \"\"\"\n    mock_cursor = Mock()\n    mock_cursor.fetchone.return_value = ('Copy executed with 0 files processed.',)\n\n    with patch('snowflake_ingestion.functions.run_sql_file'):\n        # Correction: Patcher le logger dans le module load_to_table directement\n        with patch('snowflake_ingestion.load_to_table.logger') as mock_logger:\n            table_schema = [(\"vendorid\", \"NUMBER\"), (\"tpep_pickup_datetime\", \"TIMESTAMP_NTZ\")]\n            result = load.copy_file_to_table_and_count(mock_cursor, \"test_file.parquet\", table_schema)\n            assert result == 0\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_create_table_no_schema","title":"<code>test_create_table_no_schema()</code>","text":"<p>Tests the behavior when the stage contains no data. Verifies that only the schema detection script runs, a warning is logged, no table creation SQL is executed, and an empty schema list is returned.</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_create_table_no_schema():\n    \"\"\"Tests the behavior when the stage contains no data.\n    Verifies that only the schema detection script runs, a warning is logged,\n    no table creation SQL is executed, and an empty schema list is returned.\n    \"\"\"\n    mock_cursor = Mock()\n    mock_cursor.fetchall.return_value = []\n\n    with patch('snowflake_ingestion.functions.run_sql_file') as mock_run_sql:\n        # Correction: Patcher le logger dans le module load_to_table directement\n        with patch('snowflake_ingestion.load_to_table.logger') as mock_logger:\n            result = load.create_table(mock_cursor)\n\n            mock_run_sql.assert_called_once_with(mock_cursor, load.SQL_DIR / \"detect_file_schema_stage.sql\")\n            mock_cursor.fetchall.assert_called_once()\n            mock_logger.warning.assert_called_with(\"\u26a0\ufe0f  No data in STAGE\")\n            mock_cursor.execute.assert_not_called()\n            create_sequence_call = call(mock_cursor, load.SQL_DIR / \"create_sequence.sql\")\n            add_filename_call = call(mock_cursor, load.SQL_DIR / \"add_filename_to_raw_table.sql\")\n            assert create_sequence_call not in mock_run_sql.call_args_list\n            assert add_filename_call not in mock_run_sql.call_args_list\n            assert result == []\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_create_table_success","title":"<code>test_create_table_success()</code>","text":"<p>Tests the successful creation of a table with dynamic schema detection. Verifies that all SQL files are executed in the correct order, the correct CREATE TABLE statement is generated, appropriate logs are recorded, and the correct table schema is returned.</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_create_table_success():\n    \"\"\"Tests the successful creation of a table with dynamic schema detection.\n    Verifies that all SQL files are executed in the correct order,\n    the correct CREATE TABLE statement is generated, appropriate logs are recorded,\n    and the correct table schema is returned.\n    \"\"\"\n    mock_cursor = Mock()\n    mock_cursor.fetchall.return_value = [\n        (\"vendor_id\", \"NUMBER\"),\n        (\"tpep_pickup_datetime\", \"TIMESTAMP_NTZ\"),\n        (\"tpep_dropoff_datetime\", \"TIMESTAMP_NTZ\")\n    ]\n\n    with patch('snowflake_ingestion.functions.run_sql_file') as mock_run_sql:\n        # Correction: Patcher le logger dans le module load_to_table directement\n        with patch('snowflake_ingestion.load_to_table.logger') as mock_logger:\n            result = load.create_table(mock_cursor)\n\n            assert mock_run_sql.call_args_list[0] == call(mock_cursor, load.SQL_DIR / \"detect_file_schema_stage.sql\")\n            assert mock_run_sql.call_args_list[1] == call(mock_cursor, load.SQL_DIR / \"create_sequence.sql\")\n            assert mock_run_sql.call_args_list[2] == call(mock_cursor, load.SQL_DIR / \"add_filename_to_raw_table.sql\")\n\n            mock_cursor.execute.assert_called_once()\n            create_call = mock_cursor.execute.call_args[0][0]\n            assert \"CREATE TABLE IF NOT EXISTS\" in create_call\n            assert \"vendor_id NUMBER\" in create_call\n            assert \"tpep_pickup_datetime TIMESTAMP_NTZ\" in create_call\n\n            mock_logger.info.assert_any_call(f\"\ud83d\udccb Dynamic verification/creation of table {load.functions.RAW_TABLE}\")\n            mock_logger.info.assert_any_call(f\"\u2705 Table {load.functions.RAW_TABLE} ready\")\n\n            assert result == [\n                (\"vendor_id\", \"NUMBER\"),\n                (\"tpep_pickup_datetime\", \"TIMESTAMP_NTZ\"),\n                (\"tpep_dropoff_datetime\", \"TIMESTAMP_NTZ\")\n            ]\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_handle_loading_error","title":"<code>test_handle_loading_error()</code>","text":"<p>Tests the error handling flow when a file fails to load. Verifies that an error is logged, the metadata table is updated to 'FAILED_LOAD', and a debug log is recorded.</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_handle_loading_error():\n    \"\"\"Tests the error handling flow when a file fails to load.\n    Verifies that an error is logged, the metadata table is updated to 'FAILED_LOAD',\n    and a debug log is recorded.\n    \"\"\"\n    mock_cursor = Mock()\n    test_error = Exception(\"COPY INTO failed\")\n    # Correction: Patcher le logger dans le module load_to_table directement\n    with patch('snowflake_ingestion.load_to_table.logger') as mock_logger:\n        load.handle_loading_error(mock_cursor, \"test_file.parquet\", test_error)\n        mock_logger.error.assert_called_with(f\"\u274c Loading error test_file.parquet: COPY INTO failed\")\n        mock_logger.debug.assert_called_with(f\"\ud83d\ude80 Loading {load.functions.METADATA_TABLE}\")\n        mock_cursor.execute.assert_called_once()\n        update_call = mock_cursor.execute.call_args\n        assert \"FAILED_LOAD\" in update_call[0][0]\n        assert update_call[0][1] == (\"test_file.parquet\",)\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_main_complete_flow_with_counts","title":"<code>test_main_complete_flow_with_counts()</code>","text":"<p>Tests a complete successful flow with a single file, verifying precise function call sequence. Ensures update_metadata and cleanup_stage_file are called exactly once with the correct arguments.</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_main_complete_flow_with_counts():\n    \"\"\"Tests a complete successful flow with a single file, verifying precise function call sequence.\n    Ensures update_metadata and cleanup_stage_file are called exactly once with the correct arguments.\n    \"\"\"\n    mock_conn = MagicMock()\n    mock_cursor = MagicMock()\n    mock_conn.cursor.return_value.__enter__.return_value = mock_cursor\n    mock_cursor.fetchall.return_value = [(\"file1.parquet\",)]\n\n    with patch('snowflake_ingestion.functions.connect_with_role', return_value=mock_conn):\n        with patch('snowflake_ingestion.functions.use_context'):\n            with patch('snowflake_ingestion.load_to_table.create_table', return_value=[(\"vendorid\", \"NUMBER\")]):\n                with patch('snowflake_ingestion.functions.run_sql_file'):\n                    with patch('snowflake_ingestion.load_to_table.copy_file_to_table_and_count', return_value=150):\n                        with patch('snowflake_ingestion.load_to_table.update_metadata') as mock_update:\n                            with patch('snowflake_ingestion.load_to_table.cleanup_stage_file') as mock_cleanup:\n                                # Correction: Patcher le logger dans le module load_to_table directement\n                                with patch('snowflake_ingestion.load_to_table.logger') as mock_logger:\n                                    load.main()\n                                    mock_update.assert_called_once_with(ANY, \"file1.parquet\", 150)\n                                    mock_cleanup.assert_called_once_with(ANY, \"file1.parquet\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_main_connection_error","title":"<code>test_main_connection_error()</code>","text":"<p>Tests the main flow when the initial database connection fails. Verifies that the connection exception propagates and stops the process.</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_main_connection_error():\n    \"\"\"Tests the main flow when the initial database connection fails.\n    Verifies that the connection exception propagates and stops the process.\n    \"\"\"\n    with patch('snowflake_ingestion.functions.connect_with_role') as mock_connect:\n        mock_connect.side_effect = Exception(\"Connection failed\")\n        with pytest.raises(Exception, match=\"Connection failed\"):\n            load.main()\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_main_exception_handling_in_loop","title":"<code>test_main_exception_handling_in_loop()</code>","text":"<p>Tests error handling within the file processing loop. Verifies that an error on one file triggers handle_loading_error for that file, while other files continue processing normally (update and cleanup are called for successful files).</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_main_exception_handling_in_loop():\n    \"\"\"Tests error handling within the file processing loop.\n    Verifies that an error on one file triggers handle_loading_error for that file,\n    while other files continue processing normally (update and cleanup are called for successful files).\n    \"\"\"\n    mock_conn = MagicMock()\n    mock_cursor = MagicMock()\n    mock_conn.cursor.return_value.__enter__.return_value = mock_cursor\n    mock_cursor.fetchall.return_value = [(\"file1.parquet\",), (\"file2.parquet\",), (\"file3.parquet\",)]\n\n    with patch('snowflake_ingestion.functions.connect_with_role', return_value=mock_conn):\n        with patch('snowflake_ingestion.functions.use_context'):\n            with patch('snowflake_ingestion.load_to_table.create_table', return_value=[(\"vendorid\", \"NUMBER\")]):\n                with patch('snowflake_ingestion.functions.run_sql_file'):\n                    with patch('snowflake_ingestion.load_to_table.copy_file_to_table_and_count') as mock_copy:\n                        with patch('snowflake_ingestion.load_to_table.update_metadata') as mock_update:\n                            with patch('snowflake_ingestion.load_to_table.cleanup_stage_file') as mock_cleanup:\n                                with patch('snowflake_ingestion.load_to_table.handle_loading_error') as mock_handle_error:\n                                    # Correction: Patcher le logger dans le module load_to_table directement\n                                    with patch('snowflake_ingestion.load_to_table.logger') as mock_logger:\n                                        def mock_copy_side_effect(cur, filename, table_schema):\n                                            if filename == \"file2.parquet\":\n                                                raise Exception(\"Error on file2\")\n                                            return 100\n                                        mock_copy.side_effect = mock_copy_side_effect\n                                        load.main()\n                                        assert mock_copy.call_count == 3\n                                        mock_handle_error.assert_called_once_with(ANY, \"file2.parquet\", ANY)\n                                        assert mock_update.call_count == 2\n                                        assert mock_cleanup.call_count == 2\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_main_multiple_files_different_results","title":"<code>test_main_multiple_files_different_results()</code>","text":"<p>Tests processing multiple files with different row counts. Verifies that update_metadata is called for each file with its respective row count.</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_main_multiple_files_different_results():\n    \"\"\"Tests processing multiple files with different row counts.\n    Verifies that update_metadata is called for each file with its respective row count.\n    \"\"\"\n    mock_conn = MagicMock()\n    mock_cursor = MagicMock()\n    mock_conn.cursor.return_value.__enter__.return_value = mock_cursor\n    mock_cursor.fetchall.return_value = [(\"small_file.parquet\",), (\"large_file.parquet\",)]\n\n    with patch('snowflake_ingestion.functions.connect_with_role', return_value=mock_conn):\n        with patch('snowflake_ingestion.functions.use_context'):\n            with patch('snowflake_ingestion.load_to_table.create_table', return_value=[(\"vendorid\", \"NUMBER\")]):\n                with patch('snowflake_ingestion.functions.run_sql_file'):\n                    with patch('snowflake_ingestion.load_to_table.copy_file_to_table_and_count') as mock_copy:\n                        with patch('snowflake_ingestion.load_to_table.update_metadata') as mock_update:\n                            with patch('snowflake_ingestion.load_to_table.cleanup_stage_file'):\n                                # Correction: Patcher le logger dans le module load_to_table directement\n                                with patch('snowflake_ingestion.load_to_table.logger') as mock_logger:\n                                    mock_copy.side_effect = [50, 1000]\n                                    load.main()\n                                    update_calls = mock_update.call_args_list\n                                    assert len(update_calls) == 2\n                                    assert update_calls[0][0] == (ANY, \"small_file.parquet\", 50)\n                                    assert update_calls[1][0] == (ANY, \"large_file.parquet\", 1000)\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_main_no_staged_files","title":"<code>test_main_no_staged_files()</code>","text":"<p>Tests the main flow when no files are found in the stage. Verifies that the stage analysis log occurs but no copy operations are attempted.</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_main_no_staged_files():\n    \"\"\"Tests the main flow when no files are found in the stage.\n    Verifies that the stage analysis log occurs but no copy operations are attempted.\n    \"\"\"\n    mock_conn = MagicMock()\n    mock_cursor = MagicMock()\n    mock_conn.cursor.return_value.__enter__.return_value = mock_cursor\n    mock_cursor.fetchall.return_value = []\n\n    with patch('snowflake_ingestion.functions.connect_with_role', return_value=mock_conn):\n        with patch('snowflake_ingestion.functions.use_context'):\n            with patch('snowflake_ingestion.load_to_table.create_table', return_value=[(\"vendorid\", \"NUMBER\")]):\n                with patch('snowflake_ingestion.functions.run_sql_file'):\n                    # Correction: Patcher le logger dans le module load_to_table directement\n                    with patch('snowflake_ingestion.load_to_table.logger') as mock_logger:\n                        load.main()\n                        mock_logger.info.assert_any_call(\"\ud83d\udd0d Analyzing files in STAGE\")\n                        assert not any(\"COPY INTO\" in str(call) for call in mock_cursor.execute.call_args_list)\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_main_success_flow","title":"<code>test_main_success_flow()</code>","text":"<p>Tests the complete successful main execution flow with two files. Verifies the full sequence: connection, context setting, table creation, fetching staged files, loading each file, updating metadata, and cleanup.</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_main_success_flow():\n    \"\"\"Tests the complete successful main execution flow with two files.\n    Verifies the full sequence: connection, context setting, table creation,\n    fetching staged files, loading each file, updating metadata, and cleanup.\n    \"\"\"\n    mock_conn = MagicMock()\n    mock_cursor = MagicMock()\n    mock_conn.cursor.return_value.__enter__.return_value = mock_cursor\n    mock_cursor.fetchall.side_effect = [\n        [(\"file1.parquet\",), (\"file2.parquet\",)],\n    ]\n\n    with patch('snowflake_ingestion.functions.connect_with_role', return_value=mock_conn):\n        with patch('snowflake_ingestion.functions.use_context'):\n            with patch('snowflake_ingestion.load_to_table.create_table', return_value=[(\"vendorid\", \"NUMBER\")]):\n                with patch('snowflake_ingestion.functions.run_sql_file'):\n                    with patch('snowflake_ingestion.load_to_table.copy_file_to_table_and_count') as mock_copy:\n                        with patch('snowflake_ingestion.load_to_table.update_metadata'):\n                            with patch('snowflake_ingestion.load_to_table.cleanup_stage_file'):\n                                # Correction: Patcher le logger dans le module load_to_table directement\n                                with patch('snowflake_ingestion.load_to_table.logger') as mock_logger:\n                                    mock_copy.return_value = 100\n                                    load.main()\n                                    mock_logger.info.assert_any_call(\"\ud83d\udd0d Analyzing files in STAGE\")\n                                    assert mock_copy.call_count == 2\n                                    mock_copy.assert_any_call(ANY, \"file1.parquet\", [(\"vendorid\", \"NUMBER\")])\n                                    mock_copy.assert_any_call(ANY, \"file2.parquet\", [(\"vendorid\", \"NUMBER\")])\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_main_table_creation_error","title":"<code>test_main_table_creation_error()</code>","text":"<p>Tests the main flow when table creation fails. Verifies that the exception from create_table propagates and stops the main process.</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_main_table_creation_error():\n    \"\"\"Tests the main flow when table creation fails.\n    Verifies that the exception from create_table propagates and stops the main process.\n    \"\"\"\n    mock_conn = MagicMock()\n    mock_cursor = MagicMock()\n    mock_conn.cursor.return_value.__enter__.return_value = mock_cursor\n\n    with patch('snowflake_ingestion.functions.connect_with_role', return_value=mock_conn):\n        with patch('snowflake_ingestion.functions.use_context'):\n            with patch('snowflake_ingestion.load_to_table.create_table') as mock_create_table:\n                mock_create_table.side_effect = Exception(\"Table creation failed\")\n                with pytest.raises(Exception, match=\"Table creation failed\"):\n                    load.main()\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_main_with_loading_error","title":"<code>test_main_with_loading_error()</code>","text":"<p>Tests the main flow when one file fails to load. Verifies that the error handler is called for the failed file, while successful files continue processing normally.</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_main_with_loading_error():\n    \"\"\"Tests the main flow when one file fails to load.\n    Verifies that the error handler is called for the failed file,\n    while successful files continue processing normally.\n    \"\"\"\n    mock_conn = MagicMock()\n    mock_cursor = MagicMock()\n    mock_conn.cursor.return_value.__enter__.return_value = mock_cursor\n    mock_cursor.fetchall.return_value = [(\"file1.parquet\",), (\"file2.parquet\",)]\n\n    with patch('snowflake_ingestion.functions.connect_with_role', return_value=mock_conn):\n        with patch('snowflake_ingestion.functions.use_context'):\n            with patch('snowflake_ingestion.load_to_table.create_table', return_value=[(\"vendorid\", \"NUMBER\")]):\n                with patch('snowflake_ingestion.functions.run_sql_file'):\n                    with patch('snowflake_ingestion.load_to_table.copy_file_to_table_and_count') as mock_copy:\n                        with patch('snowflake_ingestion.load_to_table.update_metadata'):\n                            with patch('snowflake_ingestion.load_to_table.cleanup_stage_file'):\n                                with patch('snowflake_ingestion.load_to_table.handle_loading_error') as mock_handle_error:\n                                    # Correction: Patcher le logger dans le module load_to_table directement\n                                    with patch('snowflake_ingestion.load_to_table.logger') as mock_logger:\n                                        def mock_copy_side_effect(cur, filename, table_schema):\n                                            if filename == \"file1.parquet\":\n                                                return 100\n                                            else:\n                                                raise Exception(\"COPY INTO failed\")\n                                        mock_copy.side_effect = mock_copy_side_effect\n                                        load.main()\n                                        assert mock_copy.call_count == 2\n                                        mock_handle_error.assert_called_once_with(ANY, \"file2.parquet\", ANY)\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_load_to_table.test_update_metadata","title":"<code>test_update_metadata()</code>","text":"<p>Tests the successful update of the metadata table after a file load. Verifies the correct UPDATE SQL is executed with the proper parameters and that a debug log is recorded.</p> Source code in <code>snowflake_ingestion/tests/test_load_to_table.py</code> <pre><code>def test_update_metadata():\n    \"\"\"Tests the successful update of the metadata table after a file load.\n    Verifies the correct UPDATE SQL is executed with the proper parameters\n    and that a debug log is recorded.\n    \"\"\"\n    mock_cursor = Mock()\n    # Correction: Patcher le logger dans le module load_to_table directement\n    with patch('snowflake_ingestion.load_to_table.logger') as mock_logger:\n        load.update_metadata(mock_cursor, \"test_file.parquet\", 250)\n        mock_cursor.execute.assert_called_once()\n        update_call = mock_cursor.execute.call_args\n        assert \"UPDATE\" in update_call[0][0]\n        assert \"rows_loaded\" in update_call[0][0]\n        assert \"SUCCESS\" in update_call[0][0]\n        assert len(update_call[0][1]) == 2\n        assert update_call[0][1][0] == 250\n        assert update_call[0][1][1] == \"test_file.parquet\"\n        mock_logger.debug.assert_called_with(f\"\ud83d\ude80 Loading {load.functions.METADATA_TABLE}\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_backup_policy","title":"<code>snowflake_ingestion.tests.test_backup_policy</code>","text":""},{"location":"docstrings/#snowflake_ingestion.tests.test_backup_policy.test_backup_configuration_values","title":"<code>test_backup_configuration_values()</code>","text":"<p>Test backup configuration constants are properly accessed and logged.</p> <p>Ensures retention days for different backup policies are correctly formatted in log messages.</p> Source code in <code>snowflake_ingestion/tests/test_backup_policy.py</code> <pre><code>def test_backup_configuration_values():\n    \"\"\"\n    Test backup configuration constants are properly accessed and logged.\n\n    Ensures retention days for different backup policies are correctly\n    formatted in log messages.\n    \"\"\"\n    mock_cursor = Mock()\n\n    with patch('snowflake_ingestion.backup_policy.functions.run_sql_file'):\n        with patch('snowflake_ingestion.backup_policy.logger') as mock_logger:\n            with patch('snowflake_ingestion.backup_policy.functions.DW_NAME', 'PRODUCTION_DW'):\n                with patch('snowflake_ingestion.backup_policy.functions.RAW_TABLE', 'YELLOW_TAXI'):\n                    with patch('snowflake_ingestion.backup_policy.functions.FINAL_SCHEMA', 'ANALYTICS'):\n                        with patch('snowflake_ingestion.backup_policy.functions.FULL_BACKUP_POLICY_DAYS', 365):\n                            with patch('snowflake_ingestion.backup_policy.functions.RAW_TABLE_BACKUP_POLICY_DAYS', 730):\n                                with patch('snowflake_ingestion.backup_policy.functions.FINAL_SCHEMA_BACKUP_POLICY_DAYS', 180):\n\n                                    backup.create_and_set_backup(mock_cursor)\n\n                                    mock_logger.info.assert_any_call(\"\u2705 PRODUCTION_DW_BACKUP retention : 365\")\n                                    mock_logger.info.assert_any_call(\"\u2705 YELLOW_TAXI_BACKUP retention : 730\")\n                                    mock_logger.info.assert_any_call(\"\u2705 ANALYTICS_BACKUP retention : 180\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_backup_policy.test_create_and_set_backup","title":"<code>test_create_and_set_backup()</code>","text":"<p>Test the create_and_set_backup function execution.</p> <p>Verifies that the function runs the correct SQL file and logs appropriate messages with retention period information for each backup policy.</p> Source code in <code>snowflake_ingestion/tests/test_backup_policy.py</code> <pre><code>def test_create_and_set_backup():\n    \"\"\"\n    Test the create_and_set_backup function execution.\n\n    Verifies that the function runs the correct SQL file and logs appropriate\n    messages with retention period information for each backup policy.\n    \"\"\"\n    mock_cursor = Mock()\n\n    with patch('snowflake_ingestion.backup_policy.functions.run_sql_file') as mock_run_sql:\n        with patch('snowflake_ingestion.backup_policy.logger') as mock_logger:\n            with patch('snowflake_ingestion.backup_policy.functions.DW_NAME', 'TEST_DW'):\n                with patch('snowflake_ingestion.backup_policy.functions.RAW_TABLE', 'TEST_RAW'):\n                    with patch('snowflake_ingestion.backup_policy.functions.FINAL_SCHEMA', 'TEST_FINAL'):\n                        with patch('snowflake_ingestion.backup_policy.functions.FULL_BACKUP_POLICY_DAYS', 90):\n                            with patch('snowflake_ingestion.backup_policy.functions.RAW_TABLE_BACKUP_POLICY_DAYS', 180):\n                                with patch('snowflake_ingestion.backup_policy.functions.FINAL_SCHEMA_BACKUP_POLICY_DAYS', 365):\n\n                                    backup.create_and_set_backup(mock_cursor)\n\n                                    expected_sql_file = backup.SQL_DIR / \"create_and_set_backup.sql\"\n                                    mock_run_sql.assert_called_once_with(mock_cursor, expected_sql_file)\n\n                                    mock_logger.info.assert_any_call(\"\ud83d\udd10 Creating backup policies and sets...\")\n                                    mock_logger.info.assert_any_call(\"\u2705 TEST_DW_BACKUP retention : 90\")\n                                    mock_logger.info.assert_any_call(\"\u2705 TEST_RAW_BACKUP retention : 180\")\n                                    mock_logger.info.assert_any_call(\"\u2705 TEST_FINAL_BACKUP retention : 365\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_backup_policy.test_main_exception","title":"<code>test_main_exception()</code>","text":"<p>Test error handling in main function when an exception occurs.</p> <p>Verifies that exceptions are caught and logged as errors without crashing.</p> Source code in <code>snowflake_ingestion/tests/test_backup_policy.py</code> <pre><code>def test_main_exception():\n    \"\"\"\n    Test error handling in main function when an exception occurs.\n\n    Verifies that exceptions are caught and logged as errors without crashing.\n    \"\"\"\n    test_exception = Exception(\"Connection failed: Invalid credentials\")\n\n    with patch('snowflake_ingestion.backup_policy.functions.connect_with_role', side_effect=test_exception):\n        with patch('snowflake_ingestion.backup_policy.logger') as mock_logger:\n\n            backup.main()\n            mock_logger.error.assert_called_once_with(test_exception)\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_backup_policy.test_main_success","title":"<code>test_main_success()</code>","text":"<p>Test the main function with successful backup setup workflow.</p> <p>Ensures connection is made with SYSADMIN role, backup function is called, connection is closed properly, and success is logged.</p> Source code in <code>snowflake_ingestion/tests/test_backup_policy.py</code> <pre><code>def test_main_success():\n    \"\"\"\n    Test the main function with successful backup setup workflow.\n\n    Ensures connection is made with SYSADMIN role, backup function is called,\n    connection is closed properly, and success is logged.\n    \"\"\"\n    mock_conn = Mock()\n    mock_cursor = Mock()\n\n    mock_conn.cursor.return_value.__enter__ = Mock(return_value=mock_cursor)\n    mock_conn.cursor.return_value.__exit__ = Mock(return_value=None)\n\n    with patch('snowflake_ingestion.backup_policy.functions.connect_with_role', return_value=mock_conn) as mock_connect:\n        with patch('snowflake_ingestion.backup_policy.create_and_set_backup') as mock_create_backup:\n            with patch('snowflake_ingestion.backup_policy.logger') as mock_logger:\n\n                backup.main()\n\n                mock_connect.assert_called_once_with(\n                    backup.functions.USER,\n                    backup.functions.PASSWORD,\n                    backup.functions.ACCOUNT,\n                    \"SYSADMIN\"\n                )\n\n                mock_create_backup.assert_called_once_with(mock_cursor)\n                mock_conn.close.assert_called_once()\n                mock_logger.info.assert_called_with(\"\ud83c\udfaf Complete initialization finished successfully!\")\n</code></pre>"},{"location":"docstrings/#snowflake_ingestion.tests.test_backup_policy.test_main_with_connection_context","title":"<code>test_main_with_connection_context()</code>","text":"<p>Test proper cursor context management in main function.</p> <p>Verifies that cursor context manager protocols are followed correctly.</p> Source code in <code>snowflake_ingestion/tests/test_backup_policy.py</code> <pre><code>def test_main_with_connection_context():\n    \"\"\"\n    Test proper cursor context management in main function.\n\n    Verifies that cursor context manager protocols are followed correctly.\n    \"\"\"\n    mock_conn = Mock()\n    mock_cursor_context = Mock()\n    mock_cursor = Mock()\n\n    mock_cursor_context.__enter__ = Mock(return_value=mock_cursor)\n    mock_cursor_context.__exit__ = Mock(return_value=None)\n    mock_conn.cursor.return_value = mock_cursor_context\n\n    with patch('snowflake_ingestion.backup_policy.functions.connect_with_role', return_value=mock_conn):\n        with patch('snowflake_ingestion.backup_policy.create_and_set_backup'):\n            with patch('snowflake_ingestion.backup_policy.logger'):\n\n                backup.main()\n                mock_conn.cursor.assert_called_once()\n                mock_cursor_context.__enter__.assert_called_once()\n                mock_cursor_context.__exit__.assert_called_once()\n</code></pre>"},{"location":"execution/","title":"\ud83d\udcbb Project Setup","text":""},{"location":"execution/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<ul> <li>Snowflake account with SECURITYADMIN and SYSADMIN privileges</li> <li>GitHub repository with configured secrets (see configuration section)</li> <li>Access to NYC Taxi data sources: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page </li> </ul>"},{"location":"execution/#execution","title":"\ud83d\ude80 Execution","text":"<ul> <li>Automatic: Every 1st of the month at 10:00 AM</li> <li>Manual: Via GitHub Actions interface </li> </ul>"},{"location":"execution/#configuration","title":"\u2699\ufe0f Configuration","text":"<ol> <li> <p>Fork this repository: https://github.com/EliasMez/nyc-taxi-pipeline </p> </li> <li> <p>Add the MANDATORY secrets: <code>Settings</code> &gt; <code>Secrets and variables</code> &gt; <code>Actions</code> &gt; <code>Secrets</code> &gt; <code>New repository secret</code> </p> </li> </ol> Secret Description <code>SNOWFLAKE_USER</code> Snowflake username <code>SNOWFLAKE_PASSWORD</code> Snowflake user password <code>SNOWFLAKE_ACCOUNT</code> Snowflake account identifier <code>PASSWORD_DEV</code> Development user password <code>PASSWORD_BI</code> BI Analyst user password <code>PASSWORD_DS</code> Data Scientist user password <code>PASSWORD_MC</code> Mart Consumer user password <code>GH_RELEASE_TOKEN</code> GitHub token for automatic versioning (required only if using the Release workflow) <p>\u26a0\ufe0f Release Workflow (Semantic Release) The Release workflow requires a GitHub token (<code>GH_RELEASE_TOKEN</code>) to function. If this token is not defined, the workflow will systematically fail during the publishing step.</p> <p>Option 1: Disable the Release workflow If you do not need automatic code versioning: <code>Actions</code> \u2192 <code>Release</code> \u2192 Disable workflow</p> <p>Option 2: Create a Personal Access Token (recommended if keeping the workflow) 1. Go to: <code>Settings</code> \u2192 <code>Developer settings</code> \u2192 <code>Personal access tokens</code> \u2192 Tokens (classic) 2. Create a token with <code>repo</code> permissions 3. Add it as a secret: <code>Settings</code> \u2192 <code>Secrets and variables</code> \u2192 <code>Actions</code> \u2192 New repository secret    - Name: <code>GH_RELEASE_TOKEN</code>    - Value: your token </p> <ol> <li>Customize the OPTIONAL variables: <code>Settings</code> &gt; <code>Secrets and variables</code> &gt; <code>Actions</code> &gt; <code>Variables</code> &gt; <code>New repository variables</code> </li> </ol> Variable Description Default Value <code>WH_NAME</code> Data warehouse name <code>NYC_WH</code> <code>DW_NAME</code> Database name <code>NYC_TAXI_DW</code> <code>RAW_SCHEMA</code> Raw data schema <code>RAW</code> <code>STAGING_SCHEMA</code> Cleaned data schema <code>STAGING</code> <code>FINAL_SCHEMA</code> Final data schema <code>FINAL</code> <code>PARQUET_FORMAT</code> Parquet file format <code>PARQUET_FORMAT</code> <code>COMPUTE_SIZE</code> Computing power of the data warehouse <code>X-SMALL</code> <code>ROLE_TRANSFORMER</code> Role for transformations <code>TRANSFORMER</code> <code>ROLE_BI_ANALYST</code> BI Analyst role name <code>ROLE_BI_ANALYST</code> <code>ROLE_DATA_SCIENTIST</code> Data Scientist role name <code>ROLE_DATA_SCIENTIST</code> <code>ROLE_MART_CONSUMER</code> Mart Consumer role name <code>ROLE_MART_CONSUMER</code> <code>USER_DEV</code> Development user <code>USER_DEV</code> <code>USER_BI_ANALYST</code> BI Analyst username <code>USER_BI_ANALYST</code> <code>USER_DATA_SCIENTIST</code> Data Scientist username <code>USER_DATA_SCIENTIST</code> <code>USER_MART_CONSUMER</code> Mart Consumer username <code>USER_MART_CONSUMER</code> <code>METADATA_TABLE</code> Metadata table <code>FILE_LOADING_METADATA</code> <code>RAW_TABLE</code> Raw data table <code>YELLOW_TAXI_TRIPS_RAW</code> <code>STAGING_TABLE</code> Staging table <code>YELLOW_TAXI_TRIPS_STG</code> <code>LOGGER_LEVEL</code> Logging level <code>INFO</code> <code>SCRAPING_YEAR</code> Scraping start date (&gt;2000 and &lt;current year) current year <code>TIMEZONE</code> Timezone defining the offset from UTC <code>UTC</code> <code>RETENTION_DAYS</code> Retention period for table change history (Time Travel) (0-90) <code>1</code> <code>FULL_BACKUP_POLICY_DAYS</code> Full database backup retention duration <code>180</code> <code>RAW_TABLE_BACKUP_POLICY_DAYS</code> RAW table backup retention duration <code>730</code> <code>FINAL_SCHEMA_BACKUP_POLICY_DAYS</code> FINAL schema backup retention duration <code>90</code> <p>\u26a0\ufe0f Important considerations regarding <code>RETENTION_DAYS</code>: *   Not applicable to Temporary tables (deleted at the end of a session). *   Fail-safe is a protection period that begins after Time Travel expires. It is not affected by this setting.</p> <p>Error behavior and limits *   \u26a0\ufe0f Automatic capping (transient tables): Any <code>RETENTION_DAYS</code> value &gt; 1 is treated as 1 day. *   \u274c Limit exceeded error: Any <code>RETENTION_DAYS</code> value exceeding the allowed limit for the account and table type will generate an error.</p>"},{"location":"execution/#standard-account","title":"Standard Account","text":"<ul> <li>Transient and permanent tables: <code>RETENTION_DAYS</code> = 0 or 1 day.</li> <li>Fail-safe: Fixed 7 days after Time Travel.</li> </ul>"},{"location":"execution/#enterprise-business-critical-and-virtual-private-snowflake-accounts","title":"Enterprise, Business Critical, and Virtual Private Snowflake Accounts","text":"<ul> <li>Transient tables: <code>RETENTION_DAYS</code> = 0 or 1 day.</li> <li>Permanent tables: <code>RETENTION_DAYS</code> = 0 to 90 days.</li> <li>Fail-safe: 7 days after Time Travel. Can be extended up to 90 days via a specific contract with Snowflake. </li> </ul>"},{"location":"execution/#quick-troubleshooting","title":"\ud83d\udd27 Quick Troubleshooting","text":"<ul> <li>Snowflake connection failure: Check GitHub secrets</li> <li>Scraping timeout: Verify access to source URLs</li> <li>dbt error: Consult detailed job logs</li> <li>Set the value of the <code>LOGGER_LEVEL</code> variable to <code>DEBUG</code> to see detailed logs</li> </ul>"},{"location":"governance/","title":"\ud83d\udcc8 Data Governance","text":""},{"location":"governance/#monitoring","title":"\ud83d\udcca Monitoring","text":"<ul> <li>Detailed logs in GitHub Actions.</li> <li>Email alerts in case of workflow failure or cancellation.</li> <li>Status tracking via a metadata table indicating each stage (scraped / staged / success / failed).</li> </ul>"},{"location":"governance/#data-quality","title":"\u2705 Data Quality","text":"<ul> <li>dbt tests ensuring data integrity, consistency, and validity.</li> <li>Duplicate management through systematic metadata verification.</li> </ul>"},{"location":"governance/#code-quality","title":"\ud83e\uddea Code Quality","text":"<ul> <li>Unit tests with Pytest.</li> <li>SQL validation with SQLFluff.</li> <li>Docstrings and doctests for function documentation.</li> <li>\ud83d\udcda Technical documentation</li> </ul>"},{"location":"governance/#security","title":"\ud83d\udd10 Security","text":"<ul> <li>Secrets encrypted in logs.</li> <li>Use of GitHub Secrets.</li> <li>Minimal permissions applied in Snowflake.</li> <li>Static analysis with CodeQL.</li> <li>Automated security updates via Dependabot.</li> </ul>"},{"location":"","title":"NYC Taxi Data Pipeline","text":"<p>This GitHub Actions workflow automates the end-to-end data pipeline, from initializing the Snowflake infrastructure to producing analytical tables and views using Python and dbt.</p> <p> \ud83d\udcbb Project source code \ud83d\udcda Online dbt documentation </p>"},{"location":"#data-source","title":"\ud83d\udcca Data Source","text":"<p>TLC Trip Record Data - NYC Taxi and Limousine Commission</p> <p>The data includes:</p> <ul> <li>Pickup and dropoff dates/times</li> <li>GPS trip locations</li> <li>Distances, detailed fares, payment types</li> <li>Passenger count reported by the driver</li> </ul> <p>The data is collected by authorized technology providers and provided to the TLC. The TLC does not guarantee the accuracy of this data.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License. The source data is provided by the NYC TLC and subject to their terms of use.</p>"},{"location":"fr/architecture/#architecture-technique","title":"\ud83c\udfd7\ufe0f Architecture Technique","text":"<ul> <li>Orchestration : GitHub Actions</li> <li>Data Warehouse : Snowflake</li> <li>Transformation : dbt</li> <li>Langage : Python </li> </ul>"},{"location":"fr/architecture/#structure-du-projet","title":"\ud83d\udcc1 Structure du Projet","text":"<pre><code>nyc-taxi-pipeline/\n\u251c\u2500\u2500 .github/\n\u2502 \u251c\u2500\u2500 workflows/\n\u2502 \u2502 \u251c\u2500\u2500 nyc_taxi_pipeline.yml\n\u2502 \u2502 \u251c\u2500\u2500 codeql.yml\n\u2502 \u2502 \u251c\u2500\u2500 python_code_tests.yml\n\u2502 \u2502 \u251c\u2500\u2500 release.yml\n\u2502 \u2502 \u2514\u2500\u2500 sqlfluff.yml\n\u2502 \u2502\n\u2502 \u2514\u2500\u2500 dependabot.yml\n\u2502\n\u251c\u2500\u2500 docs/\n\u2502\n\u251c\u2500\u2500 snowflake_ingestion/\n\u2502 \u251c\u2500\u2500 init_data_warehouse.py\n\u2502 \u251c\u2500\u2500 scrape_links.py\n\u2502 \u251c\u2500\u2500 upload_stage.py\n\u2502 \u251c\u2500\u2500 load_to_table.py\n\u2502 \u2502\n\u2502 \u251c\u2500\u2500 sql/\n\u2502 \u2502 \u251c\u2500\u2500 init/\n\u2502 \u2502 \u251c\u2500\u2500 scraping/\n\u2502 \u2502 \u251c\u2500\u2500 stage/\n\u2502 \u2502 \u2514\u2500\u2500 load/\n\u2502 \u2502\n\u2502 \u2514\u2500\u2500 tests/\n\u2502\n\u2514\u2500\u2500 dbt_transformations/\n  \u2514\u2500\u2500 NYC_Taxi_dbt/\n    \u2514\u2500\u2500 models/\n      \u251c\u2500\u2500 staging/\n      \u251c\u2500\u2500 final/\n      \u2514\u2500\u2500 marts/\n</code></pre>"},{"location":"fr/architecture/#flux-de-traitement","title":"\ud83d\udcca Flux de traitement","text":""},{"location":"fr/architecture/#pipeline-principal","title":"Pipeline Principal","text":"<p>NYC Taxi Data Pipeline Pipeline d'ingestion ex\u00e9cut\u00e9 mensuellement : </p> <ol> <li>Snowflake Infra Init    Initialisation de l'infrastructure Snowflake (base, sch\u00e9mas, warehouse, r\u00f4le, utilisateur).</li> <li>Scrape Links    Scraping et r\u00e9cup\u00e9ration des liens sources.</li> <li>Upload to Stage    Upload des fichiers bruts dans le stage Snowflake.</li> <li>Load to Table    Chargement des donn\u00e9es dans la table du sch\u00e9ma RAW.</li> <li>Run dbt Transformations    Transformations dbt (STAGING puis FINAL).</li> <li>Run dbt Tests    Ex\u00e9cution des tests dbt pour valider les mod\u00e8les.</li> <li>Backup Policy    Configuration automatique des politiques de sauvegarde pour la base, table RAW et sch\u00e9ma FINAL.</li> </ol>"},{"location":"fr/architecture/#pipelines-qualite","title":"Pipelines Qualit\u00e9","text":"<ul> <li>CodeQL Security Scan  Analyse statique du code Python \u00e0 l\u2019aide de CodeQL afin de d\u00e9tecter des vuln\u00e9rabilit\u00e9s sur chaque push ou pull request vers <code>dev</code> et <code>main</code>.</li> <li>Dependabot Updates  Mises \u00e0 jour automatis\u00e9es des d\u00e9pendances Python et GitHub Actions selon une planification trimestrielle.</li> <li>pages-build-deployment  D\u00e9ploiement automatique de la documentation du projet via GitHub Pages.</li> <li>Python Code Tests  Ex\u00e9cution des tests unitaires Pytest sur chaque push ou pull request vers <code>dev</code> et <code>main</code>.</li> <li>Release  Versioning automatique, g\u00e9n\u00e9ration du changelog et publication des releases via Python Semantic Release sur chaque push ou pull request vers <code>main</code>.</li> <li>SQL Code Quality  Linting automatique du code SQL (mod\u00e8les dbt et scripts Snowflake) avec SQLFluff sur chaque push ou pull request vers <code>dev</code> et <code>main</code>.</li> </ul>"},{"location":"fr/architecture/#modelisation-des-donnees","title":"Mod\u00e9lisation des Donn\u00e9es","text":"<p>Le tableau documente comment les donn\u00e9es sont stock\u00e9es.</p> Nom de la table Sch\u00e9ma Type de table Mat\u00e9rialisation FILE_LOADING_METADATA <code>SCHEMA_RAW</code> Transitoire Table YELLOW_TAXI_TRIPS_RAW <code>SCHEMA_RAW</code> Permanente Incremental TAXI_ZONE_LOOKUP <code>SCHEMA_RAW</code> Permanente Table TAXI_ZONE_STG <code>SCHEMA_STG</code> Transitoire Table YELLOW_TAXI_TRIPS_STG <code>SCHEMA_STG</code> Transitoire Incremental int_trip_metrics <code>SCHEMA_STG</code> Vue fact_trips <code>SCHEMA_FINAL</code> Permanente Incremental dim_locations <code>SCHEMA_FINAL</code> Permanente Table dim_time <code>SCHEMA_FINAL</code> Permanente Table dim_date <code>SCHEMA_FINAL</code> Permanente Table marts <code>SCHEMA_FINAL</code> Vue <p>details disponibles dans la \ud83d\udcda Documentation dbt en ligne</p>"},{"location":"fr/execution/","title":"\ud83d\udcbb D\u00e9marrage du Projet","text":""},{"location":"fr/execution/#prerequis","title":"\ud83d\udccb Pr\u00e9requis","text":"<ul> <li>Compte Snowflake avec droits SECURITYADMIN et SYSADMIN</li> <li>D\u00e9p\u00f4t GitHub avec secrets configur\u00e9s (voir partie configuration)</li> <li>Acc\u00e8s sources de donn\u00e9es NYC Taxi : https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page </li> </ul>"},{"location":"fr/execution/#execution","title":"\ud83d\ude80 Ex\u00e9cution","text":"<ul> <li>Automatique : tous les 1 du mois \u00e0 10h</li> <li>Manuel : via GitHub Actions interface </li> </ul>"},{"location":"fr/execution/#configuration","title":"\u2699\ufe0f Configuration","text":"<ol> <li> <p>Forkez ce d\u00e9p\u00f4t : https://github.com/EliasMez/nyc-taxi-pipeline </p> </li> <li> <p>Ajoutez les secrets OBLIGATOIRES : <code>Settings</code> &gt; <code>Secrets and variables</code> &gt; <code>Actions</code> &gt; <code>Secrets</code> &gt; <code>New repository secret</code> </p> </li> </ol> Secret Description <code>SNOWFLAKE_USER</code> Nom d'utilisateur Snowflake <code>SNOWFLAKE_PASSWORD</code> Mot de passe utilisateur Snowflake <code>SNOWFLAKE_ACCOUNT</code> Identifiant du compte Snowflake <code>PASSWORD_DEV</code> Mot de passe de l'utilisateur de d\u00e9veloppement <code>PASSWORD_BI</code> Mot de passe utilisateur Analyste BI <code>PASSWORD_DS</code> Mot de passe utilisateur Data Scientist <code>PASSWORD_MC</code> Mot de passe utilisateur Consommateur Mart <code>GH_RELEASE_TOKEN</code> Token GitHub pour le versionnement automatique (n\u00e9cessaire seulement si vous utilisez le workflow Release) <p>\u26a0\ufe0f Workflow Release (Semantic Release) Le workflow Release n\u00e9cessite un token GitHub (<code>GH_RELEASE_TOKEN</code>) pour fonctionner. Si ce token n\u2019est pas d\u00e9fini, le workflow \u00e9chouera syst\u00e9matiquement lors de l\u2019\u00e9tape de publication.</p> <p>Option 1 : D\u00e9sactiver le workflow Release Si vous n\u2019avez pas besoin du versionnement automatique de code : <code>Actions</code> \u2192 <code>Release</code> \u2192 Disable workflow</p> <p>Option 2 : Cr\u00e9er un Personal Access Token (recommand\u00e9 si vous gardez le workflow) 1. Allez dans :  <code>Settings</code> \u2192 <code>Developer settings</code> \u2192 <code>Personal access tokens</code> \u2192 Tokens (classic) 2. Cr\u00e9ez un token avec les permissions <code>repo</code> 3. Ajoutez-le comme secret : <code>Settings</code> \u2192 <code>Secrets and variables</code> \u2192 <code>Actions</code> \u2192 New repository secret    - Nom : <code>GH_RELEASE_TOKEN</code>    - Valeur : votre token </p> <ol> <li>Personnalisez les variables OPTIONNELLES : <code>Settings</code> &gt; <code>Secrets and variables</code> &gt; <code>Actions</code> &gt; <code>Variables</code> &gt; <code>New repository variables</code> </li> </ol> Variable Description Valeur par d\u00e9faut <code>WH_NAME</code> Nom du data warehouse <code>NYC_WH</code> <code>DW_NAME</code> Nom de la base de donn\u00e9es <code>NYC_TAXI_DW</code> <code>RAW_SCHEMA</code> Sch\u00e9ma des donn\u00e9es brutes <code>RAW</code> <code>STAGING_SCHEMA</code> Sch\u00e9ma des donn\u00e9es nettoy\u00e9es <code>STAGING</code> <code>FINAL_SCHEMA</code> Sch\u00e9ma des donn\u00e9es finales <code>FINAL</code> <code>PARQUET_FORMAT</code> Format de fichier Parquet <code>PARQUET_FORMAT</code> <code>COMPUTE_SIZE</code> Puissance de calcul de l'entrep\u00f4t de donn\u00e9es <code>X-SMALL</code> <code>ROLE_TRANSFORMER</code> R\u00f4le pour les transformations <code>TRANSFORMER</code> <code>ROLE_BI_ANALYST</code> Nom du r\u00f4le Analyste BI <code>ROLE_DATA_SCIENTIST</code> Nom du r\u00f4le Data Scientist <code>ROLE_DATA_SCIENTIST</code> <code>ROLE_MART_CONSUMER</code> Nom du r\u00f4le Consommateur Mart <code>ROLE_MART_CONSUMER</code> <code>USER_DEV</code> Utilisateur de d\u00e9veloppement <code>USER_DEV</code> <code>USER_BI_ANALYST</code> Nom d'utilisateur Analyste BI <code>USER_BI_ANALYST</code> <code>USER_DATA_SCIENTIST</code> Nom d'utilisateur Data Scientist <code>USER_DATA_SCIENTIST</code> <code>USER_MART_CONSUMER</code> Nom d'utilisateur Consommateur Mart <code>USER_MART_CONSUMER</code> <code>METADATA_TABLE</code> Table de m\u00e9tadonn\u00e9es <code>FILE_LOADING_METADATA</code> <code>RAW_TABLE</code> Table des donn\u00e9es brutes <code>YELLOW_TAXI_TRIPS_RAW</code> <code>STAGING_TABLE</code> Table de staging <code>YELLOW_TAXI_TRIPS_STG</code> <code>LOGGER_LEVEL</code> Niveau de logging <code>INFO</code> <code>SCRAPING_YEAR</code> Date de d\u00e9but du scraping (&gt;2000 et &lt;ann\u00e9e courante) ann\u00e9e courante <code>TIMEZONE</code> Fuseau horaire qui d\u00e9finit le d\u00e9calage horaire par rapport \u00e0 UTC <code>UTC</code> <code>RETENTION_DAYS</code> Dur\u00e9e de conservation de l'historique des modifications des tables (Time Travel) (0-90) <code>1</code> <code>FULL_BACKUP_POLICY_DAYS</code> Dur\u00e9e de conservation des sauvegardes compl\u00e8tes de la base de donn\u00e9es <code>180</code> <code>RAW_TABLE_BACKUP_POLICY_DAYS</code> Dur\u00e9e de conservation des sauvegardes de la table RAW <code>730</code> <code>FINAL_SCHEMA_BACKUP_POLICY_DAYS</code> Dur\u00e9e de conservation des sauvegardes du sch\u00e9ma FINAL <code>90</code> <p></p> <p>\u26a0\ufe0f Consid\u00e9rations importantes concernant RETENTION_DAYS: Non applicable sur les Tables temporaires (supprim\u00e9es en fin de session). Fail-safe est une p\u00e9riode de protection qui commence apr\u00e8s l'expiration du Time Travel. Elle n'est pas affect\u00e9e par ce param\u00e8tre.</p> <p>Comportement des erreurs et plafonds *   \u26a0\ufe0f Plafonnement automatique (tables transitoires) : Toute valeur de <code>RETENTION_DAYS</code> &gt; 1 est trait\u00e9e comme 1 jour. *   \u274c Erreur de d\u00e9passement de limite : Toute valeur de <code>RETENTION_DAYS</code> d\u00e9passant la limite autoris\u00e9e pour le type de compte et de table g\u00e9n\u00e8re une erreur.</p> <p>Compte Standard *   Tables transitoires et permanentes : <code>RETENTION_DAYS</code> = 0 ou 1 jour. *   Fail-safe : 7 jours fixe apr\u00e8s Time Travel.</p> <p>Comptes Enterprise, Business Critical et Virtual Private Snowflake *   Tables transitoires : <code>RETENTION_DAYS</code> = 0 ou 1 jour. *   Tables permanentes : <code>RETENTION_DAYS</code> = 0 \u00e0 90 jours. *   Fail-safe : 7 jours apr\u00e8s Time Travel. Peut \u00eatre \u00e9tendu jusqu'\u00e0 90 jours via un contrat sp\u00e9cifique avec Snowflake. </p>"},{"location":"fr/execution/#depannage-rapide","title":"\ud83d\udd27 D\u00e9pannage Rapide","text":"<ul> <li>\u00c9chec connexion Snowflake : V\u00e9rifier les secrets GitHub</li> <li>Timeout scraping : V\u00e9rifier l'acc\u00e8s aux URLs sources</li> <li>Erreur dbt : Consulter les logs d\u00e9taill\u00e9s du job</li> <li>Passer la valeur de la variable <code>LOGGER_LEVEL</code> \u00e0 <code>DEBUG</code> pour voir les logs d\u00e9taill\u00e9s</li> </ul>"},{"location":"fr/governance/","title":"\ud83d\udcc8 Gouvernance des donn\u00e9es","text":""},{"location":"fr/governance/#monitoring","title":"\ud83d\udcca Monitoring","text":"<ul> <li>Logs d\u00e9taill\u00e9s dans GitHub Actions.  </li> <li>Alertes e-mail en cas d\u2019\u00e9chec ou d\u2019annulation du workflow.  </li> <li>Suivi de l\u2019\u00e9tat via une table de m\u00e9tadonn\u00e9es indiquant chaque \u00e9tape (scraped / staged / success / failed).</li> </ul>"},{"location":"fr/governance/#qualite-des-donnees","title":"\u2705 Qualit\u00e9 des donn\u00e9es","text":"<ul> <li>Tests dbt garantissant l\u2019int\u00e9grit\u00e9, la coh\u00e9rence et la validit\u00e9 des donn\u00e9es.  </li> <li>Gestion des doublons via une v\u00e9rification syst\u00e9matique des m\u00e9tadonn\u00e9es.</li> </ul>"},{"location":"fr/governance/#qualite-du-code","title":"\ud83e\uddea Qualit\u00e9 du code","text":"<ul> <li>Tests unitaires avec Pytest.  </li> <li>Validation SQL avec SQLFluff.  </li> <li>Docstrings et doctests pour la documentation des fonctions.  </li> <li>\ud83d\udcda Documentation technique</li> </ul>"},{"location":"fr/governance/#securite","title":"\ud83d\udd10 S\u00e9curit\u00e9","text":"<ul> <li>Secrets chiffr\u00e9s dans les logs.  </li> <li>Utilisation des GitHub Secrets.  </li> <li>Permissions minimales appliqu\u00e9es dans Snowflake.  </li> <li>Analyse statique avec CodeQL.  </li> <li>Mises \u00e0 jour de s\u00e9curit\u00e9 automatis\u00e9es via Dependabot.</li> </ul>"},{"location":"fr/","title":"NYC Taxi Data Pipeline","text":"<p>Ce workflow GitHub Actions automatise le pipeline de donn\u00e9es de bout en bout, depuis l'initialisation de l'infrastructure Snowflake jusqu'\u00e0 la production de tables et vues analytiques en utilisant python et dbt.  \ud83d\udcbb Code source du projet \ud83d\udcda Documentation dbt en ligne </p>"},{"location":"fr/#source-des-donnees","title":"\ud83d\udcca Source des Donn\u00e9es","text":"<p>TLC Trip Record Data - Commission des Taxis et Limousines de NYC</p> <p>Les donn\u00e9es incluent :</p> <ul> <li>Dates/heures de prise en charge et d\u00e9pose</li> <li>Localisations GPS des trajets</li> <li>Distances, tarifs d\u00e9taill\u00e9s, types de paiement</li> <li>Nombre de passagers rapport\u00e9 par le chauffeur</li> </ul> <p>Les donn\u00e9es sont collect\u00e9es par les fournisseurs technologiques autoris\u00e9s et fournies \u00e0 la TLC. La TLC ne garantit pas l'exactitude de ces donn\u00e9es.</p>"},{"location":"fr/#licence","title":"\ud83d\udcc4 Licence","text":"<p>Ce projet est sous licence MIT. Les donn\u00e9es source sont fournies par la NYC TLC et soumises \u00e0 leurs conditions d'utilisation.</p>"},{"location":"es/architecture/","title":"\ud83c\udfdb\ufe0f Arquitectura","text":""},{"location":"es/architecture/#arquitectura-tecnica","title":"\ud83c\udfd7\ufe0f Arquitectura T\u00e9cnica","text":"<ul> <li>Orquestaci\u00f3n: GitHub Actions</li> <li>Data Warehouse: Snowflake</li> <li>Transformaci\u00f3n: dbt</li> <li>Lenguaje: Python </li> </ul>"},{"location":"es/architecture/#estructura-del-proyecto","title":"\ud83d\udcc1 Estructura del Proyecto","text":"<pre><code>nyc-taxi-pipeline/\n\u251c\u2500\u2500 .github/\n\u2502   \u251c\u2500\u2500 workflows/\n\u2502   \u2502   \u251c\u2500\u2500 nyc_taxi_pipeline.yml\n\u2502   \u2502   \u251c\u2500\u2500 codeql.yml\n\u2502   \u2502   \u251c\u2500\u2500 python_code_tests.yml\n\u2502   \u2502   \u251c\u2500\u2500 release.yml\n\u2502   \u2502   \u2514\u2500\u2500 sqlfluff.yml\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 dependabot.yml\n\u2502\n\u251c\u2500\u2500 docs/\n\u2502\n\u251c\u2500\u2500 snowflake_ingestion/\n\u2502   \u251c\u2500\u2500 init_data_warehouse.py\n\u2502   \u251c\u2500\u2500 scrape_links.py\n\u2502   \u251c\u2500\u2500 upload_stage.py\n\u2502   \u251c\u2500\u2500 load_to_table.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 sql/\n\u2502   \u2502   \u251c\u2500\u2500 init/\n\u2502   \u2502   \u251c\u2500\u2500 scraping/\n\u2502   \u2502   \u251c\u2500\u2500 stage/\n\u2502   \u2502   \u2514\u2500\u2500 load/\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 tests/\n\u2502\n\u2514\u2500\u2500 dbt_transformations/\n    \u2514\u2500\u2500 NYC_Taxi_dbt/\n        \u2514\u2500\u2500 models/\n            \u251c\u2500\u2500 staging/\n            \u251c\u2500\u2500 final/\n            \u2514\u2500\u2500 marts/\n</code></pre>"},{"location":"es/architecture/#flujo-de-procesamiento","title":"\ud83d\udcca Flujo de Procesamiento","text":""},{"location":"es/architecture/#pipeline-principal","title":"Pipeline Principal","text":"<p>NYC Taxi Data Pipeline Pipeline de ingesti\u00f3n de datos ejecutado mensualmente: </p> <ol> <li>Inicializaci\u00f3n de Infraestructura Snowflake    Inicializaci\u00f3n de la infraestructura de Snowflake (base de datos, esquemas, warehouse, rol, usuario).</li> <li>Recolecci\u00f3n de Enlaces    Web scraping y recuperaci\u00f3n de enlaces de origen.</li> <li>Carga a Stage    Carga de archivos crudos al stage de Snowflake.</li> <li>Carga a Tabla    Carga de datos en la tabla del esquema RAW.</li> <li>Ejecuci\u00f3n de Transformaciones dbt    Transformaciones dbt (STAGING luego FINAL).</li> <li>Ejecuci\u00f3n de Pruebas dbt    Ejecuci\u00f3n de pruebas dbt para validar los modelos.</li> <li>Pol\u00edtica de Copias de Seguridad    Configuraci\u00f3n autom\u00e1tica de pol\u00edticas de respaldo para la base de datos, tabla RAW y esquema FINAL.</li> </ol>"},{"location":"es/architecture/#pipelines-de-calidad","title":"Pipelines de Calidad","text":"<ul> <li>CodeQL Security Scan   An\u00e1lisis est\u00e1tico del c\u00f3digo Python usando CodeQL para detectar vulnerabilidades en cada push o pull request a <code>dev</code> y <code>main</code>.</li> <li>Actualizaciones Dependabot   Actualizaciones automatizadas de dependencias de Python y GitHub Actions seg\u00fan un calendario trimestral.</li> <li>pages-build-deployment   Implementaci\u00f3n autom\u00e1tica de la documentaci\u00f3n del proyecto a trav\u00e9s de GitHub Pages.</li> <li>Pruebas de C\u00f3digo Python   Ejecuci\u00f3n de pruebas unitarias Pytest en cada push o pull request a <code>dev</code> y <code>main</code>.</li> <li>Release   Versionado autom\u00e1tico, generaci\u00f3n de changelog y publicaci\u00f3n de releases mediante Python Semantic Release en cada push o pull request a <code>main</code>.</li> <li>Calidad de C\u00f3digo SQL   Linting autom\u00e1tico del c\u00f3digo SQL (modelos dbt y scripts de Snowflake) con SQLFluff en cada push o pull request a <code>dev</code> y <code>main</code>.</li> </ul>"},{"location":"es/architecture/#modelado-de-datos-data-modeling","title":"Modelado de Datos (Data Modeling)","text":"<p>Esta tabla documenta c\u00f3mo se almacenan los datos.</p> Nombre de la Tabla Esquema Tipo de Tabla Materializaci\u00f3n FILE_LOADING_METADATA <code>SCHEMA_RAW</code> Transitoria Tabla YELLOW_TAXI_TRIPS_RAW <code>SCHEMA_RAW</code> Permanente Incremental TAXI_ZONE_LOOKUP <code>SCHEMA_RAW</code> Permanente Tabla TAXI_ZONE_STG <code>SCHEMA_STG</code> Transitoria Tabla YELLOW_TAXI_TRIPS_STG <code>SCHEMA_STG</code> Transitoria Incremental int_trip_metrics <code>SCHEMA_STG</code> Vista fact_trips <code>SCHEMA_FINAL</code> Permanente Incremental dim_locations <code>SCHEMA_FINAL</code> Permanente Tabla dim_time <code>SCHEMA_FINAL</code> Permanente Tabla dim_date <code>SCHEMA_FINAL</code> Permanente Tabla marts <code>SCHEMA_FINAL</code> Vista <p>Detalles disponibles en la \ud83d\udcda Documentaci\u00f3n en l\u00ednea de dbt</p>"},{"location":"es/execution/","title":"\ud83d\udcbb Inicio del Proyecto","text":""},{"location":"es/execution/#requisitos-previos","title":"\ud83d\udccb Requisitos Previos","text":"<ul> <li>Cuenta de Snowflake con privilegios SECURITYADMIN y SYSADMIN</li> <li>Repositorio de GitHub con secretos configurados (ver secci\u00f3n de configuraci\u00f3n)</li> <li>Acceso a las fuentes de datos de NYC Taxi: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page </li> </ul>"},{"location":"es/execution/#ejecucion","title":"\ud83d\ude80 Ejecuci\u00f3n","text":"<ul> <li>Autom\u00e1tica: Cada d\u00eda 1 del mes a las 10:00</li> <li>Manual: A trav\u00e9s de la interfaz de GitHub Actions </li> </ul>"},{"location":"es/execution/#configuracion","title":"\u2699\ufe0f Configuraci\u00f3n","text":"<ol> <li> <p>Haz un fork de este repositorio: https://github.com/EliasMez/nyc-taxi-pipeline </p> </li> <li> <p>A\u00f1ade los secretos OBLIGATORIOS: <code>Settings</code> &gt; <code>Secrets and variables</code> &gt; <code>Actions</code> &gt; <code>Secrets</code> &gt; <code>New repository secret</code> </p> </li> </ol> Secreto Descripci\u00f3n <code>SNOWFLAKE_USER</code> Nombre de usuario de Snowflake <code>SNOWFLAKE_PASSWORD</code> Contrase\u00f1a del usuario de Snowflake <code>SNOWFLAKE_ACCOUNT</code> Identificador de la cuenta de Snowflake <code>PASSWORD_DEV</code> Contrase\u00f1a del usuario de desarrollo <code>PASSWORD_BI</code> Contrase\u00f1a de usuario Analista BI <code>PASSWORD_DS</code> Contrase\u00f1a de usuario Cient\u00edfico de Datos <code>PASSWORD_MC</code> Contrase\u00f1a de usuario Consumidor de Marts <code>GH_RELEASE_TOKEN</code> Token de GitHub para el versionado autom\u00e1tico (necesario solo si usa el workflow Release) <p>\u26a0\ufe0f Workflow de Lanzamiento (Semantic Release) El workflow Release requiere un token de GitHub (<code>GH_RELEASE_TOKEN</code>) para funcionar. Si este token no est\u00e1 definido, el workflow fallar\u00e1 sistem\u00e1ticamente durante el paso de publicaci\u00f3n.</p> <p>Opci\u00f3n 1: Desactivar el workflow Release Si no necesita el versionado autom\u00e1tico de c\u00f3digo: <code>Actions</code> \u2192 <code>Release</code> \u2192 Disable workflow</p> <p>Opci\u00f3n 2: Crear un Personal Access Token (recomendado si mantiene el workflow) 1. Ve a: <code>Settings</code> \u2192 <code>Developer settings</code> \u2192 <code>Personal access tokens</code> \u2192 Tokens (classic) 2. Crea un token con permisos <code>repo</code> 3. A\u00f1\u00e1delo como secreto: <code>Settings</code> \u2192 <code>Secrets and variables</code> \u2192 <code>Actions</code> \u2192 New repository secret    - Nombre: <code>GH_RELEASE_TOKEN</code>    - Valor: su token </p> <ol> <li>Personaliza las variables OPCIONALES: <code>Settings</code> &gt; <code>Secrets and variables</code> &gt; <code>Actions</code> &gt; <code>Variables</code> &gt; <code>New repository variables</code> </li> </ol> Variable Descripci\u00f3n Valor por Defecto <code>WH_NAME</code> Nombre del almac\u00e9n de datos <code>NYC_WH</code> <code>DW_NAME</code> Nombre de la base de datos <code>NYC_TAXI_DW</code> <code>RAW_SCHEMA</code> Esquema de datos crudos <code>RAW</code> <code>STAGING_SCHEMA</code> Esquema de datos limpiados <code>STAGING</code> <code>FINAL_SCHEMA</code> Esquema de datos finales <code>FINAL</code> <code>PARQUET_FORMAT</code> Formato de archivo Parquet <code>PARQUET_FORMAT</code> <code>COMPUTE_SIZE</code> Potencia de c\u00e1lculo del almac\u00e9n de datos <code>X-SMALL</code> <code>ROLE_TRANSFORMER</code> Rol para las transformaciones <code>TRANSFORMER</code> <code>ROLE_BI_ANALYST</code> Nombre del rol Analista BI <code>ROLE_BI_ANALYST</code> <code>ROLE_DATA_SCIENTIST</code> Nombre del rol Cient\u00edfico de Datos <code>ROLE_DATA_SCIENTIST</code> <code>ROLE_MART_CONSUMER</code> Nombre del rol Consumidor de Marts <code>ROLE_MART_CONSUMER</code> <code>USER_DEV</code> Usuario de desarrollo <code>USER_DEV</code> <code>USER_BI_ANALYST</code> Nombre de usuario Analista BI <code>USER_BI_ANALYST</code> <code>USER_DATA_SCIENTIST</code> Nombre de usuario Cient\u00edfico de Datos <code>USER_DATA_SCIENTIST</code> <code>USER_MART_CONSUMER</code> Nombre de usuario Consumidor de Marts <code>USER_MART_CONSUMER</code> <code>METADATA_TABLE</code> Tabla de metadatos <code>FILE_LOADING_METADATA</code> <code>RAW_TABLE</code> Tabla de datos crudos <code>YELLOW_TAXI_TRIPS_RAW</code> <code>STAGING_TABLE</code> Tabla de staging <code>YELLOW_TAXI_TRIPS_STG</code> <code>LOGGER_LEVEL</code> Nivel de registro <code>INFO</code> <code>SCRAPING_YEAR</code> Fecha de inicio del scraping (&gt;2000 y &lt;a\u00f1o actual) a\u00f1o actual <code>TIMEZONE</code> Zona horaria que define el desplazamiento respecto a UTC <code>UTC</code> <code>RETENTION_DAYS</code> Per\u00edodo de retenci\u00f3n del historial de cambios de tablas (Time Travel) (0-90) <code>1</code> <code>FULL_BACKUP_POLICY_DAYS</code> Duraci\u00f3n de retenci\u00f3n de copias de seguridad completas de la base de datos <code>180</code> <code>RAW_TABLE_BACKUP_POLICY_DAYS</code> Duraci\u00f3n de retenci\u00f3n de copias de seguridad de la tabla RAW <code>730</code> <code>FINAL_SCHEMA_BACKUP_POLICY_DAYS</code> Duraci\u00f3n de retenci\u00f3n de copias de seguridad del esquema FINAL <code>90</code> <p></p> <p>\u26a0\ufe0f Consideraciones importantes sobre <code>RETENTION_DAYS</code>: *   No aplicable a las Tablas temporales (eliminadas al final de una sesi\u00f3n). *   Fail-safe es un per\u00edodo de protecci\u00f3n que comienza despu\u00e9s de que expire Time Travel. No se ve afectado por esta configuraci\u00f3n.</p> <p>\ud83d\udccc Comportamiento de errores y l\u00edmites *   \u26a0\ufe0f Limitaci\u00f3n autom\u00e1tica (tablas transitorias): Cualquier valor de <code>RETENTION_DAYS</code> &gt; 1 se trata como 1 d\u00eda. *   \u274c Error de l\u00edmite excedido: Cualquier valor de <code>RETENTION_DAYS</code> que exceda el l\u00edmite permitido para el tipo de cuenta y tabla generar\u00e1 un error.</p>"},{"location":"es/execution/#cuenta-estandar","title":"Cuenta Est\u00e1ndar","text":"<ul> <li>Tablas transitorias y permanentes: <code>RETENTION_DAYS</code> = 0 o 1 d\u00eda.</li> <li>Fail-safe: 7 d\u00edas fijos despu\u00e9s de Time Travel.</li> </ul>"},{"location":"es/execution/#cuentas-enterprise-business-critical-y-virtual-private-snowflake","title":"Cuentas Enterprise, Business Critical y Virtual Private Snowflake","text":"<ul> <li>Tablas transitorias: <code>RETENTION_DAYS</code> = 0 o 1 d\u00eda.</li> <li>Tablas permanentes: <code>RETENTION_DAYS</code> = 0 a 90 d\u00edas.</li> <li>Fail-safe: 7 d\u00edas despu\u00e9s de Time Travel. Se puede extender hasta 90 d\u00edas mediante un contrato espec\u00edfico con Snowflake. </li> </ul>"},{"location":"es/execution/#solucion-rapida-de-problemas","title":"\ud83d\udd27 Soluci\u00f3n R\u00e1pida de Problemas","text":"<ul> <li>Fallo de conexi\u00f3n con Snowflake: Verificar los secretos de GitHub</li> <li>Timeout del scraping: Verificar el acceso a las URLs fuente</li> <li>Error de dbt: Consultar los registros detallados del job</li> <li>Cambie el valor de la variable <code>LOGGER_LEVEL</code> a <code>DEBUG</code> para ver registros detallados</li> </ul>"},{"location":"es/governance/","title":"\ud83d\udcc8 Gobernanza de Datos","text":""},{"location":"es/governance/#monitoreo","title":"\ud83d\udcca Monitoreo","text":"<ul> <li>Registros detallados en GitHub Actions.</li> <li>Alertas por correo electr\u00f3nico en caso de falla o cancelaci\u00f3n del flujo de trabajo.</li> <li>Seguimiento del estado a trav\u00e9s de una tabla de metadatos que indica cada etapa (raspado / almacenado provisionalmente / \u00e9xito / fallido).</li> </ul>"},{"location":"es/governance/#calidad-de-datos","title":"\u2705 Calidad de Datos","text":"<ul> <li>Pruebas de dbt que garantizan la integridad, coherencia y validez de los datos.</li> <li>Gesti\u00f3n de duplicados mediante verificaci\u00f3n sistem\u00e1tica de metadatos.</li> </ul>"},{"location":"es/governance/#calidad-del-codigo","title":"\ud83e\uddea Calidad del C\u00f3digo","text":"<ul> <li>Pruebas unitarias con Pytest.</li> <li>Validaci\u00f3n SQL con SQLFluff.</li> <li>Cadenas de documentaci\u00f3n y pruebas de documentaci\u00f3n para la documentaci\u00f3n de funciones.</li> <li>\ud83d\udcda Documentaci\u00f3n t\u00e9cnica</li> </ul>"},{"location":"es/governance/#seguridad","title":"\ud83d\udd10 Seguridad","text":"<ul> <li>Secretos cifrados en los registros.</li> <li>Uso de GitHub Secrets.</li> <li>Permisos m\u00ednimos aplicados en Snowflake.</li> <li>An\u00e1lisis est\u00e1tico con CodeQL.</li> <li>Actualizaciones de seguridad automatizadas a trav\u00e9s de Dependabot.</li> </ul>"},{"location":"es/","title":"NYC Taxi Data Pipeline","text":"<p>Este flujo de trabajo de GitHub Actions automatiza la canalizaci\u00f3n de datos de extremo a extremo, desde la inicializaci\u00f3n de la infraestructura de Snowflake hasta la producci\u00f3n de tablas y vistas anal\u00edticas utilizando Python y dbt.</p> <p> \ud83d\udcbb C\u00f3digo fuente del proyecto \ud83d\udcda Documentaci\u00f3n dbt en l\u00ednea </p>"},{"location":"es/#fuente-de-datos","title":"\ud83d\udcca Fuente de Datos","text":"<p>Datos de Registro de Viajes de la TLC - Comisi\u00f3n de Taxis y Limusinas de Nueva York</p> <p>Los datos incluyen:</p> <ul> <li>Fechas/horas de recogida y entrega</li> <li>Ubicaciones GPS de los viajes</li> <li>Distancias, tarifas detalladas, tipos de pago</li> <li>N\u00famero de pasajeros reportado por el conductor</li> </ul> <p>Los datos son recopilados por proveedores de tecnolog\u00eda autorizados y proporcionados a la TLC. La TLC no garantiza la precisi\u00f3n de estos datos.</p>"},{"location":"es/#licencia","title":"\ud83d\udcc4 Licencia","text":"<p>Este proyecto est\u00e1 bajo la licencia MIT. Los datos de origen son proporcionados por la NYC TLC y sujetos a sus t\u00e9rminos de uso.</p>"},{"location":"zh/architecture/","title":"\ud83c\udfdb\ufe0f \u67b6\u6784","text":""},{"location":"zh/architecture/#_2","title":"\ud83c\udfd7\ufe0f \u6280\u672f\u67b6\u6784","text":"<ul> <li>\u7f16\u6392\u5de5\u5177: GitHub Actions</li> <li>\u6570\u636e\u4ed3\u5e93: Snowflake</li> <li>\u6570\u636e\u8f6c\u6362: dbt</li> <li>\u7f16\u7a0b\u8bed\u8a00: Python </li> </ul>"},{"location":"zh/architecture/#_3","title":"\ud83d\udcc1 \u9879\u76ee\u7ed3\u6784","text":"<pre><code>nyc-taxi-pipeline/\n\u251c\u2500\u2500 .github/\n\u2502   \u251c\u2500\u2500 workflows/\n\u2502   \u2502   \u251c\u2500\u2500 nyc_taxi_pipeline.yml\n\u2502   \u2502   \u251c\u2500\u2500 codeql.yml\n\u2502   \u2502   \u251c\u2500\u2500 python_code_tests.yml\n\u2502   \u2502   \u251c\u2500\u2500 release.yml\n\u2502   \u2502   \u2514\u2500\u2500 sqlfluff.yml\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 dependabot.yml\n\u2502\n\u251c\u2500\u2500 docs/\n\u2502\n\u251c\u2500\u2500 snowflake_ingestion/\n\u2502   \u251c\u2500\u2500 init_data_warehouse.py\n\u2502   \u251c\u2500\u2500 scrape_links.py\n\u2502   \u251c\u2500\u2500 upload_stage.py\n\u2502   \u251c\u2500\u2500 load_to_table.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 sql/\n\u2502   \u2502   \u251c\u2500\u2500 init/\n\u2502   \u2502   \u251c\u2500\u2500 scraping/\n\u2502   \u2502   \u251c\u2500\u2500 stage/\n\u2502   \u2502   \u2514\u2500\u2500 load/\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 tests/\n\u2502\n\u2514\u2500\u2500 dbt_transformations/\n    \u2514\u2500\u2500 NYC_Taxi_dbt/\n        \u2514\u2500\u2500 models/\n            \u251c\u2500\u2500 staging/\n            \u251c\u2500\u2500 final/\n            \u2514\u2500\u2500 marts/\n</code></pre>"},{"location":"zh/architecture/#_4","title":"\ud83d\udcca \u6570\u636e\u5904\u7406\u6d41\u7a0b","text":""},{"location":"zh/architecture/#_5","title":"\u4e3b\u8981\u6d41\u7a0b","text":"<p>\u7ebd\u7ea6\u51fa\u79df\u8f66\u6570\u636e\u7ba1\u9053 \u6bcf\u6708\u6267\u884c\u7684\u6570\u636e\u6444\u53d6\u7ba1\u9053\uff1a </p> <ol> <li>Snowflake \u57fa\u7840\u8bbe\u65bd\u521d\u59cb\u5316    \u521d\u59cb\u5316 Snowflake \u57fa\u7840\u8bbe\u65bd\uff08\u6570\u636e\u5e93\u3001\u6a21\u5f0f\u3001\u4ed3\u5e93\u3001\u89d2\u8272\u3001\u7528\u6237\uff09\u3002</li> <li>\u6293\u53d6\u94fe\u63a5    \u6293\u53d6\u5e76\u83b7\u53d6\u6e90\u94fe\u63a5\u3002</li> <li>\u4e0a\u4f20\u5230 Stage    \u5c06\u539f\u59cb\u6587\u4ef6\u4e0a\u4f20\u5230 Snowflake Stage\u3002</li> <li>\u52a0\u8f7d\u5230\u8868    \u5c06\u6570\u636e\u52a0\u8f7d\u5230 RAW \u6a21\u5f0f\u7684\u8868\u4e2d\u3002</li> <li>\u8fd0\u884c dbt \u8f6c\u6362    \u6267\u884c dbt \u8f6c\u6362\uff08STAGING \u7136\u540e\u662f FINAL\uff09\u3002</li> <li>\u8fd0\u884c dbt \u6d4b\u8bd5    \u6267\u884c dbt \u6d4b\u8bd5\u4ee5\u9a8c\u8bc1\u6a21\u578b\u3002</li> <li>\u5907\u4efd\u7b56\u7565    \u81ea\u52a8\u914d\u7f6e\u6570\u636e\u5e93\u3001RAW\u8868\u548cFINAL\u6a21\u5f0f\u7684\u5907\u4efd\u7b56\u7565\u3002</li> </ol>"},{"location":"zh/architecture/#_6","title":"\u8d28\u91cf\u6d41\u7a0b","text":"<ul> <li>CodeQL \u5b89\u5168\u626b\u63cf   \u4f7f\u7528 CodeQL \u5bf9 Python \u4ee3\u7801\u8fdb\u884c\u9759\u6001\u5206\u6790\uff0c\u4ee5\u68c0\u6d4b\u6bcf\u6b21\u63a8\u9001\u6216\u62c9\u53d6\u8bf7\u6c42\u5230 <code>dev</code> \u548c <code>main</code> \u65f6\u7684\u6f0f\u6d1e\u3002</li> <li>Dependabot \u66f4\u65b0   \u6bcf\u5b63\u5ea6\u81ea\u52a8\u66f4\u65b0 Python \u548c GitHub Actions \u4f9d\u8d56\u9879\u3002</li> <li>\u9875\u9762\u6784\u5efa\u4e0e\u90e8\u7f72   \u901a\u8fc7 GitHub Pages \u81ea\u52a8\u90e8\u7f72\u9879\u76ee\u6587\u6863\u3002</li> <li>Python \u4ee3\u7801\u6d4b\u8bd5   \u5728\u6bcf\u6b21\u63a8\u9001\u6216\u62c9\u53d6\u8bf7\u6c42\u5230 <code>dev</code> \u548c <code>main</code> \u65f6\u6267\u884c Pytest \u5355\u5143\u6d4b\u8bd5\u3002</li> <li>\u7248\u672c\u53d1\u5e03   \u901a\u8fc7 Python Semantic Release \u81ea\u52a8\u8fdb\u884c\u7248\u672c\u63a7\u5236\u3001\u751f\u6210\u53d8\u66f4\u65e5\u5fd7\u548c\u53d1\u5e03\u7248\u672c\uff0c\u6bcf\u6b21\u63a8\u9001\u6216\u62c9\u53d6\u8bf7\u6c42\u5230 <code>main</code> \u65f6\u89e6\u53d1\u3002</li> <li>SQL \u4ee3\u7801\u8d28\u91cf   \u4f7f\u7528 SQLFluff \u5bf9 SQL \u4ee3\u7801\uff08dbt \u6a21\u578b\u548c Snowflake \u811a\u672c\uff09\u8fdb\u884c\u81ea\u52a8 linting\uff0c\u6bcf\u6b21\u63a8\u9001\u6216\u62c9\u53d6\u8bf7\u6c42\u5230 <code>dev</code> \u548c <code>main</code> \u65f6\u6267\u884c\u3002 ``</li> </ul>"},{"location":"zh/architecture/#data-modeling","title":"\u6570\u636e\u5efa\u6a21 (Data Modeling)","text":"<p>\u6b64\u8868\u8bb0\u5f55\u4e86\u6570\u636e\u7684\u5b58\u50a8\u65b9\u5f0f\u3002</p> \u8868\u540d\u79f0 \u6a21\u5f0f \u8868\u7c7b\u578b \u7269\u5316\u65b9\u5f0f FILE_LOADING_METADATA <code>SCHEMA_RAW</code> \u77ac\u6001\u8868 \u8868 YELLOW_TAXI_TRIPS_RAW <code>SCHEMA_RAW</code> \u6c38\u4e45\u8868 \u589e\u91cf TAXI_ZONE_LOOKUP <code>SCHEMA_RAW</code> \u6c38\u4e45\u8868 \u8868 TAXI_ZONE_STG <code>SCHEMA_STG</code> \u77ac\u6001\u8868 \u8868 YELLOW_TAXI_TRIPS_STG <code>SCHEMA_STG</code> \u77ac\u6001\u8868 \u589e\u91cf int_trip_metrics <code>SCHEMA_STG</code> \u89c6\u56fe fact_trips <code>SCHEMA_FINAL</code> \u6c38\u4e45\u8868 \u589e\u91cf dim_locations <code>SCHEMA_FINAL</code> \u6c38\u4e45\u8868 \u8868 dim_time <code>SCHEMA_FINAL</code> \u6c38\u4e45\u8868 \u8868 dim_date <code>SCHEMA_FINAL</code> \u6c38\u4e45\u8868 \u8868 marts <code>SCHEMA_FINAL</code> \u89c6\u56fe <p>\u8be6\u7ec6\u5185\u5bb9\u53ef\u67e5\u9605 \ud83d\udcda \u5728\u7ebf dbt \u6587\u6863</p>"},{"location":"zh/execution/","title":"\ud83d\udcbb \u9879\u76ee\u542f\u52a8","text":""},{"location":"zh/execution/#_2","title":"\ud83d\udccb \u5148\u51b3\u6761\u4ef6","text":"<ul> <li>\u5177\u6709 SECURITYADMIN \u548c SYSADMIN \u6743\u9650\u7684 Snowflake \u8d26\u6237</li> <li>\u5305\u542b \u5df2\u914d\u7f6e\u5bc6\u94a5 \u7684 GitHub \u4ed3\u5e93\uff08\u89c1\u914d\u7f6e\u90e8\u5206\uff09</li> <li>\u8bbf\u95ee NYC Taxi \u6570\u636e\u6e90\uff1ahttps://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page </li> </ul>"},{"location":"zh/execution/#_3","title":"\ud83d\ude80 \u6267\u884c","text":"<ul> <li>\u81ea\u52a8\uff1a\u6bcf\u6708 1 \u65e5\u4e0a\u5348 10:00</li> <li>\u624b\u52a8\uff1a\u901a\u8fc7 GitHub Actions \u754c\u9762 </li> </ul>"},{"location":"zh/execution/#_4","title":"\u2699\ufe0f \u914d\u7f6e","text":"<ol> <li> <p>\u590d\u523b \u6b64\u4ed3\u5e93\uff1ahttps://github.com/EliasMez/nyc-taxi-pipeline </p> </li> <li> <p>\u6dfb\u52a0 \u5fc5\u9700 \u7684\u5bc6\u94a5\uff1a <code>Settings</code> &gt; <code>Secrets and variables</code> &gt; <code>Actions</code> &gt; <code>Secrets</code> &gt; <code>New repository secret</code> </p> </li> </ol> Secret \u63cf\u8ff0 <code>SNOWFLAKE_USER</code> Snowflake \u7528\u6237\u540d <code>SNOWFLAKE_PASSWORD</code> Snowflake \u7528\u6237\u5bc6\u7801 <code>SNOWFLAKE_ACCOUNT</code> Snowflake \u8d26\u6237\u6807\u8bc6\u7b26 <code>PASSWORD_DEV</code> \u5f00\u53d1\u7528\u6237\u5bc6\u7801 <code>PASSWORD_BI</code> BI\u5206\u6790\u5e08\u7528\u6237\u5bc6\u7801 <code>PASSWORD_DS</code> \u6570\u636e\u79d1\u5b66\u5bb6\u7528\u6237\u5bc6\u7801 <code>PASSWORD_MC</code> \u6570\u636e\u96c6\u5e02\u6d88\u8d39\u8005\u7528\u6237\u5bc6\u7801 <code>GH_RELEASE_TOKEN</code> \u7528\u4e8e\u81ea\u52a8\u7248\u672c\u63a7\u5236\u7684 GitHub \u4ee4\u724c\uff08\u4ec5\u5728\u9700\u8981\u4f7f\u7528 Release \u5de5\u4f5c\u6d41\u65f6\uff09 <p>\u26a0\ufe0f \u53d1\u5e03\u5de5\u4f5c\u6d41\uff08\u8bed\u4e49\u5316\u53d1\u5e03\uff09 \u53d1\u5e03 \u5de5\u4f5c\u6d41\u9700\u8981\u4e00\u4e2a GitHub \u4ee4\u724c\uff08<code>GH_RELEASE_TOKEN</code>\uff09\u624d\u80fd\u8fd0\u884c\u3002 \u5982\u679c\u672a\u5b9a\u4e49\u6b64\u4ee4\u724c\uff0c\u5de5\u4f5c\u6d41\u5c06\u5728\u53d1\u5e03\u6b65\u9aa4\u4e2d\u7cfb\u7edf\u6027\u5730\u5931\u8d25\u3002</p> <p>\u9009\u9879 1\uff1a\u7981\u7528 Release \u5de5\u4f5c\u6d41 \u5982\u679c\u60a8\u4e0d\u9700\u8981\u81ea\u52a8\u4ee3\u7801\u7248\u672c\u63a7\u5236\uff1a<code>Actions</code> \u2192 <code>Release</code> \u2192 Disable workflow</p> <p>\u9009\u9879 2\uff1a\u521b\u5efa\u4e2a\u4eba\u8bbf\u95ee\u4ee4\u724c\uff08\u63a8\u8350\uff0c\u5982\u679c\u8981\u4fdd\u7559\u8be5\u5de5\u4f5c\u6d41\uff09 1. \u524d\u5f80\uff1a<code>Settings</code> \u2192 <code>Developer settings</code> \u2192 <code>Personal access tokens</code> \u2192 Tokens (classic) 2. \u521b\u5efa\u4e00\u4e2a\u5177\u6709 <code>repo</code> \u6743\u9650\u7684\u4ee4\u724c 3. \u5c06\u5176\u6dfb\u52a0\u4e3a\u5bc6\u94a5\uff1a<code>Settings</code> \u2192 <code>Secrets and variables</code> \u2192 <code>Actions</code> \u2192 New repository secret    - \u540d\u79f0\uff1a<code>GH_RELEASE_TOKEN</code>    - \u503c\uff1a\u60a8\u7684\u4ee4\u724c </p> <ol> <li>\u81ea\u5b9a\u4e49 \u53ef\u9009 \u53d8\u91cf\uff1a <code>Settings</code> &gt; <code>Secrets and variables</code> &gt; <code>Actions</code> &gt; <code>Variables</code> &gt; <code>New repository variables</code> </li> </ol> \u53d8\u91cf \u63cf\u8ff0 \u9ed8\u8ba4\u503c <code>WH_NAME</code> \u6570\u636e\u4ed3\u5e93\u540d\u79f0 <code>NYC_WH</code> <code>DW_NAME</code> \u6570\u636e\u5e93\u540d\u79f0 <code>NYC_TAXI_DW</code> <code>RAW_SCHEMA</code> \u539f\u59cb\u6570\u636e\u6a21\u5f0f <code>RAW</code> <code>STAGING_SCHEMA</code> \u5df2\u6e05\u7406\u6570\u636e\u6a21\u5f0f <code>STAGING</code> <code>FINAL_SCHEMA</code> \u6700\u7ec8\u6570\u636e\u6a21\u5f0f <code>FINAL</code> <code>PARQUET_FORMAT</code> Parquet \u6587\u4ef6\u683c\u5f0f <code>PARQUET_FORMAT</code> <code>COMPUTE_SIZE</code> \u6570\u636e\u4ed3\u5e93\u7684\u8ba1\u7b97\u80fd\u529b <code>X-SMALL</code> <code>ROLE_TRANSFORMER</code> \u8f6c\u6362\u89d2\u8272 <code>TRANSFORMER</code> <code>ROLE_BI_ANALYST</code> BI\u5206\u6790\u5e08\u89d2\u8272\u540d\u79f0 <code>ROLE_BI_ANALYST</code> <code>ROLE_DATA_SCIENTIST</code> \u6570\u636e\u79d1\u5b66\u5bb6\u89d2\u8272\u540d\u79f0 <code>ROLE_DATA_SCIENTIST</code> <code>ROLE_MART_CONSUMER</code> \u6570\u636e\u96c6\u5e02\u6d88\u8d39\u8005\u89d2\u8272\u540d\u79f0 <code>ROLE_MART_CONSUMER</code> <code>USER_DEV</code> \u5f00\u53d1\u7528\u6237 <code>USER_DEV</code> <code>USER_BI_ANALYST</code> BI\u5206\u6790\u5e08\u7528\u6237\u540d <code>USER_BI_ANALYST</code> <code>USER_DATA_SCIENTIST</code> \u6570\u636e\u79d1\u5b66\u5bb6\u7528\u6237\u540d <code>USER_DATA_SCIENTIST</code> <code>USER_MART_CONSUMER</code> \u6570\u636e\u96c6\u5e02\u6d88\u8d39\u8005\u7528\u6237\u540d <code>USER_MART_CONSUMER</code> <code>METADATA_TABLE</code> \u5143\u6570\u636e\u8868 <code>FILE_LOADING_METADATA</code> <code>RAW_TABLE</code> \u539f\u59cb\u6570\u636e\u8868 <code>YELLOW_TAXI_TRIPS_RAW</code> <code>STAGING_TABLE</code> \u6682\u5b58\u8868 <code>YELLOW_TAXI_TRIPS_STG</code> <code>LOGGER_LEVEL</code> \u65e5\u5fd7\u8bb0\u5f55\u7ea7\u522b <code>INFO</code> <code>SCRAPING_YEAR</code> \u6293\u53d6\u5f00\u59cb\u65e5\u671f (&gt;2000 \u4e14 &lt;\u5f53\u524d\u5e74\u4efd) \u5f53\u524d\u5e74\u4efd <code>TIMEZONE</code> \u5b9a\u4e49\u4e0e UTC \u504f\u79fb\u7684\u65f6\u533a <code>UTC</code> <code>RETENTION_DAYS</code> \u8868\u66f4\u6539\u5386\u53f2\uff08\u65f6\u5149\u65c5\u884c\uff09\u7684\u4fdd\u7559\u671f (0-90) <code>1</code> <code>FULL_BACKUP_POLICY_DAYS</code> \u5b8c\u6574\u6570\u636e\u5e93\u5907\u4efd\u4fdd\u7559\u65f6\u957f <code>180</code> <code>RAW_TABLE_BACKUP_POLICY_DAYS</code> RAW\u8868\u5907\u4efd\u4fdd\u7559\u65f6\u957f <code>730</code> <code>FINAL_SCHEMA_BACKUP_POLICY_DAYS</code> FINAL\u6a21\u5f0f\u5907\u4efd\u4fdd\u7559\u65f6\u957f <code>90</code> <p>\u26a0\ufe0f \u5173\u4e8e <code>RETENTION_DAYS</code> \u7684\u91cd\u8981\u6ce8\u610f\u4e8b\u9879\uff1a *   \u4e0d\u9002\u7528\u4e8e\u4e34\u65f6\u8868\uff08\u5728\u4f1a\u8bdd\u7ed3\u675f\u65f6\u5220\u9664\uff09\u3002 *   \u6545\u969c\u5b89\u5168\u4fdd\u62a4\u662f\u5728\u65f6\u5149\u65c5\u884c\u8fc7\u671f\u540e\u5f00\u59cb\u7684\u4fdd\u62a4\u671f\u3002\u4e0d\u53d7\u6b64\u8bbe\u7f6e\u5f71\u54cd\u3002</p> <p>\u9519\u8bef\u884c\u4e3a\u4e0e\u9650\u5236 *   \u26a0\ufe0f \u81ea\u52a8\u4e0a\u9650\uff08\u77ac\u6001\u8868\uff09\uff1a\u4efb\u4f55 <code>RETENTION_DAYS</code> \u503c &gt; 1 \u5747\u88ab\u89c6\u4e3a 1 \u5929\u3002 *   \u274c \u8d85\u51fa\u9650\u5236\u9519\u8bef\uff1a\u4efb\u4f55 <code>RETENTION_DAYS</code> \u503c\u5982\u679c\u8d85\u8fc7\u8d26\u6237\u548c\u8868\u7c7b\u578b\u5141\u8bb8\u7684\u9650\u5236\uff0c\u90fd\u4f1a\u4ea7\u751f\u9519\u8bef\u3002</p>"},{"location":"zh/execution/#_5","title":"\u6807\u51c6\u8d26\u6237","text":"<ul> <li>\u77ac\u6001\u8868\u548c\u6c38\u4e45\u8868\uff1a<code>RETENTION_DAYS</code> = 0 \u6216 1 \u5929\u3002</li> <li>\u6545\u969c\u5b89\u5168\u4fdd\u62a4\uff1a\u65f6\u5149\u65c5\u884c\u540e\u56fa\u5b9a\u7684 7 \u5929\u3002</li> </ul>"},{"location":"zh/execution/#enterprisebusiness-critical-virtual-private-snowflake","title":"Enterprise\u3001Business Critical \u548c Virtual Private Snowflake \u8d26\u6237","text":"<ul> <li>\u77ac\u6001\u8868\uff1a<code>RETENTION_DAYS</code> = 0 \u6216 1 \u5929\u3002</li> <li>\u6c38\u4e45\u8868\uff1a<code>RETENTION_DAYS</code> = 0 \u81f3 90 \u5929\u3002</li> <li>\u6545\u969c\u5b89\u5168\u4fdd\u62a4\uff1a\u65f6\u5149\u65c5\u884c\u540e 7 \u5929\u3002\u53ef\u901a\u8fc7\u4e0e Snowflake \u7b7e\u8ba2\u7279\u5b9a\u5408\u540c\u5ef6\u957f\u81f3 90 \u5929\u3002 </li> </ul>"},{"location":"zh/execution/#_6","title":"\ud83d\udd27 \u5feb\u901f\u6545\u969c\u6392\u9664","text":"<ul> <li>Snowflake \u8fde\u63a5\u5931\u8d25\uff1a\u68c0\u67e5 GitHub \u5bc6\u94a5</li> <li>\u6293\u53d6\u8d85\u65f6\uff1a\u9a8c\u8bc1\u5bf9\u6e90 URL \u7684\u8bbf\u95ee</li> <li>dbt \u9519\u8bef\uff1a\u67e5\u770b\u8be6\u7ec6\u7684\u4f5c\u4e1a\u65e5\u5fd7</li> <li>\u5c06 <code>LOGGER_LEVEL</code> \u53d8\u91cf\u7684\u503c\u8bbe\u7f6e\u4e3a <code>DEBUG</code> \u4ee5\u67e5\u770b\u8be6\u7ec6\u65e5\u5fd7</li> </ul>"},{"location":"zh/governance/","title":"\ud83d\udcc8 \u6570\u636e\u6cbb\u7406","text":""},{"location":"zh/governance/#_2","title":"\ud83d\udcca \u76d1\u63a7","text":"<ul> <li>GitHub Actions \u4e2d\u7684\u8be6\u7ec6\u65e5\u5fd7\u3002</li> <li>\u5de5\u4f5c\u6d41\u5931\u8d25\u6216\u53d6\u6d88\u65f6\u7684\u7535\u5b50\u90ae\u4ef6\u8b66\u62a5\u3002</li> <li>\u901a\u8fc7\u5143\u6570\u636e\u8868\u8ddf\u8e2a\u6bcf\u4e2a\u9636\u6bb5\u7684\u72b6\u6001\uff08\u5df2\u6293\u53d6 / \u5df2\u6682\u5b58 / \u6210\u529f / \u5931\u8d25\uff09\u3002</li> </ul>"},{"location":"zh/governance/#_3","title":"\u2705 \u6570\u636e\u8d28\u91cf","text":"<ul> <li>dbt \u6d4b\u8bd5\u786e\u4fdd\u6570\u636e\u7684\u5b8c\u6574\u6027\u3001\u4e00\u81f4\u6027\u548c\u6709\u6548\u6027\u3002</li> <li>\u901a\u8fc7\u7cfb\u7edf\u7684\u5143\u6570\u636e\u9a8c\u8bc1\u8fdb\u884c\u91cd\u590d\u9879\u7ba1\u7406\u3002</li> </ul>"},{"location":"zh/governance/#_4","title":"\ud83e\uddea \u4ee3\u7801\u8d28\u91cf","text":"<ul> <li>\u4f7f\u7528 Pytest \u8fdb\u884c\u5355\u5143\u6d4b\u8bd5\u3002</li> <li>\u4f7f\u7528 SQLFluff \u8fdb\u884c SQL \u9a8c\u8bc1\u3002</li> <li>\u4f7f\u7528\u6587\u6863\u5b57\u7b26\u4e32\u548c\u6587\u6863\u6d4b\u8bd5\u8fdb\u884c\u51fd\u6570\u6587\u6863\u8bb0\u5f55\u3002</li> <li>\ud83d\udcda \u6280\u672f\u6587\u6863</li> </ul>"},{"location":"zh/governance/#_5","title":"\ud83d\udd10 \u5b89\u5168","text":"<ul> <li>\u65e5\u5fd7\u4e2d\u7684\u5bc6\u94a5\u52a0\u5bc6\u3002</li> <li>\u4f7f\u7528 GitHub Secrets\u3002</li> <li>\u5728 Snowflake \u4e2d\u5e94\u7528\u6700\u5c0f\u6743\u9650\u539f\u5219\u3002</li> <li>\u4f7f\u7528 CodeQL \u8fdb\u884c\u9759\u6001\u5206\u6790\u3002</li> <li>\u901a\u8fc7 Dependabot \u8fdb\u884c\u81ea\u52a8\u5b89\u5168\u66f4\u65b0\u3002</li> </ul>"},{"location":"zh/","title":"\u7ebd\u7ea6\u5e02\u51fa\u79df\u8f66\u6570\u636e\u7ba1\u9053","text":"<p>\u6b64 GitHub Actions \u5de5\u4f5c\u6d41\u4f7f\u7528 Python \u548c dbt \u81ea\u52a8\u5316\u7aef\u5230\u7aef\u7684\u6570\u636e\u7ba1\u9053\uff0c\u4ece\u521d\u59cb\u5316 Snowflake \u57fa\u7840\u8bbe\u65bd\u5230\u751f\u6210\u5206\u6790\u8868\u548c\u89c6\u56fe\u3002</p> <p> \ud83d\udcbb \u9879\u76ee\u6e90\u4ee3\u7801 \ud83d\udcda \u5728\u7ebf dbt \u6587\u6863 </p>"},{"location":"zh/#_2","title":"\ud83d\udcca \u6570\u636e\u6765\u6e90","text":"<p>TLC \u884c\u7a0b\u8bb0\u5f55\u6570\u636e - \u7ebd\u7ea6\u5e02\u51fa\u79df\u8f66\u548c\u793c\u8f66\u59d4\u5458\u4f1a</p> <p>\u6570\u636e\u5305\u542b\uff1a</p> <ul> <li>\u4e0a\u8f66\u548c\u4e0b\u8f66\u65e5\u671f/\u65f6\u95f4</li> <li>GPS \u884c\u7a0b\u4f4d\u7f6e</li> <li>\u8ddd\u79bb\u3001\u8be6\u7ec6\u8d39\u7528\u3001\u652f\u4ed8\u7c7b\u578b</li> <li>\u53f8\u673a\u62a5\u544a\u4e58\u5ba2\u6570\u91cf</li> </ul> <p>\u6570\u636e\u7531\u6388\u6743\u6280\u672f\u63d0\u4f9b\u5546\u6536\u96c6\u5e76\u63d0\u4f9b\u7ed9 TLC\u3002TLC \u4e0d\u4fdd\u8bc1\u6b64\u6570\u636e\u7684\u51c6\u786e\u6027\u3002</p>"},{"location":"zh/#_3","title":"\ud83d\udcc4 \u8bb8\u53ef\u8bc1","text":"<p>\u672c\u9879\u76ee\u91c7\u7528 MIT \u8bb8\u53ef\u8bc1\u3002\u6e90\u6570\u636e\u7531 \u7ebd\u7ea6\u5e02 TLC \u63d0\u4f9b\uff0c\u5e76\u53d7\u5176\u4f7f\u7528\u6761\u6b3e\u7ea6\u675f\u3002</p>"}]}